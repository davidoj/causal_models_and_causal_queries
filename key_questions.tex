
\section{Key Questions}

\section{Causal Models}

\subsubsection{Definition of Causal Models}

\begin{definition}[Causal Models]\label{def:causal_models}
Given a set $\mathbf{V}$ of random variables and a set $\mathcal{I}$ of interventions, a causal model is a map $M:\mathcal{I}\to \mathcal{P}(\mathbf{V})$ where $\mathcal{P}(\mathbf{V})$ represents the set of all probability distributions over $\mathbf{V}$.
\end{definition}

\begin{question}
    Do the major examples of causal models in the literature satisfy Definition \ref{def:causal_models}?
    \begin{itemize}
        \item Causal Bayesian Networks
        \item Structural Causal Models
        \item Potential Outcomes/Rubin Causal Model
        \item Marginal models
    \end{itemize}
\end{question}

\subsubsection{Relationships between types of causal models}

\begin{question}
    Are Causal Bayesian Networks equivalent to Structural Causal Models, or is the latter a subset of the former?
\end{question}

\begin{question}
    One feature of structural causal models is the possibility to derive deterministic maps by substituting constants for the noise variables $u_i$. This could be seen as a generalisation of interventions from operations that replace the right hand side of an assignment with a constant to an operation that makes a more general modification to the right hand side of an assignment.
    
    What is the set of general interventions on an SCM that preserve the Markovian property?
\end{question}

\begin{question}
    The basic Rubin Causal Model appears to be a general causal model as in Definition \ref{def:causal_models} with a binary intervention space $\mathcal{I}$ \cite{sontag_causal_nodate}. 
    
    In practice, the Rubin Causal Model is usually used with the assumptions of strong ignorability and stable unit treatment value. Are Rubin Causal Models that satisfy these assumptions also Causal Bayesian Networks?
\end{question}

\subsubsection{Causal models vs probability distributions}

\begin{question}
    Pearl claims there are fundamental differences between causal and probabilistic models \cite{pearl_causality:_2009}. The difference between a causal model in Definition \ref{def:causal_models} and a probability distribution is that a causal model requires the specification of an intervention or distribution over interventions in order to produce a probability distribution over observables. Is this difference sufficient to explain Pearl's dichotomy?
\end{question}

\subsubsection{Causal models vs other kinds of models}

\begin{question}
    What is the relationship between general/Markovian causal models and other system models, particularly where interaction with the system is important? 
    
    Relationship could mean: does one type of model a subset of another? Is there some operation that generates one type of model given another?
    \begin{itemize}
        \item First order state space models
        \item Markov Decision Processes
    \end{itemize}
\end{question}

\subsubsection{Markovian causal models}

\begin{question}
    Is there an appropriate way to define an "approximately Markovian" causal model?
    Points to consider:
    \begin{itemize}
        \item Are there operations we can perform on data generated by a Markovian system that will yield an approximately Markovian system? Example operations: forgetting data for some variables, averaging data over time or over repeated instances.
        \item Is there a "mathematically neat" way to define approximate Markovianity?
    \end{itemize}
\end{question}

\subsection{Causal Inference Problems}

\begin{question}\label{q:causal_queries}
    What are the key kinds of causal queries? A very preliminary list:
    \begin{itemize}
        \item Control/prediction: how would a system behave if intervention $I$ were applied?
        \item Attribution: what is responsible for the observation of outcome $v$?
        \item Latent variables: identifying latent variables causally responsible for observed data
        \item Causal structure: identifying how observed variables are causally related to one another
    \end{itemize}
\end{question}

\begin{question}
    Detecting latent variables has a clear connection to dimensionality reduction \cite{kummerfeld_causal_2016}. As a general question, how does \emph{causal} latent variable detection differ from other forms of dimension reduction?
    
    As a more specific question, is there a particular form of loss function that is, implicitly or explicitly, assumed in causal latent variable detection? 
\end{question}

\begin{question}\label{q:causal_query_difficulty}
    How should the difficulty of a causal query be measured?
    Some possibilities:
    \begin{itemize}
        \item Observational sample complexity
        \item Interventional sample complexity
    \end{itemize}
\end{question}

\begin{question}
    Is it possible to define a generic set of elements that constitute a causal inference query? For example, a control query might require:
    \begin{itemize}
        \item Outcome/objective
        \item Set of allowed interventions
        \item Set of observations
        \item Hypothesis class
        \item Assumptions
        \item Data
    \end{itemize}
\end{question}

\begin{question}
    Causal queries typically employ assumptions over and above the assumptions made for statistical queries. Are additional assumptions always necessary?
    
    More precisely: given a causal model $M$ for which the class $\mathcal{P}$ of distributions satisfies statistical learnability assumptions (clarification pending), what characterises causal queries that are unresolvable without further assumptions?
\end{question}

\begin{question}\label{q:query_set_of_models}
    Given a causal query (pending definition) and the assumption of a Markovian causal model, does the query induce equivalence classes of causal models? Does this depend on the type of query?
\end{question}

\begin{question}
    Can two identical causal models (Definition \ref{def:causal_models}) with different side information (definition pending) give different answers to a causal query (definition pending)?
\end{question}

\begin{question}
    What are key features in a taxonomy of causal inference problems that aims to group problems by which approaches to solving the problem may/may not work? Do the objects identified in Definition \ref{def:causal_models} - $\mathbf{V}$, $\mathcal{I}$, $\mathcal{P}$ play important roles in such a taxonomy?
\end{question}

\begin{question}
    Given a causal bayesian network $M$ with targets $Y$, other observables $X$ and interventions $\mathcal{I}$ and cost $C:\mathcal{I}\times Y\to\mathbb{R}$, are there control strategies possible with $M$ that may be superior to control strategies possible if only $P(Y|I)$ is known for all $I\in\mathcal{I}$?
\end{question}

\subsection{Causal Discovery}

Causal discovery describes any method of learning a set of Markovian causal models that might describe the causal relationships between variables in a dataset.

\begin{question}
    Is it correct to consider queries about causal structures instrumental queries in service of other aims? Alternatively, should causal structures be added to the list in Question \ref{q:causal_queries}?
\end{question}

Many algorithms for causal structure discovery do not resolve whether $X$ and $Y$ are confounded by an unobserved $Z$ or not. On the other hand, the answer many conceivable causal queries will depend on whether two variables are confounded or directly causally related. The next questions focus on whether structural identification can help with causal queries.

\begin{question}\label{q:discovery_set_of_models}
    A structure discovery algorithm will typically output some class $\mathcal{C}$ of causal models. In some cases these classes are well-defined, for example Markov Equivalence Classes. In some cases it is not so clear - algorithms that cannot detect latent confounders are usually introduced with the assumption that no such confounders exist. However, it is also possible to view them as outputting a large class of causal models that contains many models with latent confounders and some without.
    
    The question here is what classes of causal models are output by the various structure learning algorithms?
\end{question}

\begin{question}
    Considering the model sets in \ref{q:query_set_of_models} and \ref{q:discovery_set_of_models}, when is it possible for the output of a structure discovery algorithm to facilitate answering a causal query - for example, by reducing the query to one that is easier on a metric of difficulty (question \ref{q:causal_query_difficulty}).
\end{question}

\begin{question}
    Which causal discovery algorithms require the assumption of faithfulness?
    \begin{itemize}
        \item PC and IC definitely do
        \item Bayesian score based (e.g. Greedy Equivalence Search) I'm not sure
        \item Additive noise models, information geometric definitely don't
        \item Latent factor detection (BPC, FOFC) - I'm not sure \cite{kummerfeld_causal_2016}
    \end{itemize}
\end{question}

\begin{question}
    What is the relationship between $\lambda$ in $\lambda$-faithfulness and sample complexity of PC-type algorithms?
\end{question}

\begin{question}
    Are there weakenings of faithfulness or $\lambda$-faithfulness, and do these allow for consistency results of their own?
    \begin{itemize}
        \item Some types of faithfulness violations have testable implications, so only some types of faithfulness need to be assumed \cite{ramsey_adjacency-faithfulness_2012}
        \item Some types of faithfulness violations could be benign (e.g. violated faithfulness in $X\to Y$) \cite{peters_structural_2013}
        \item The degree to which faithfulness violations impact the answer to a causal query might scale with the number of "non-transparent" conditional independences 
    \end{itemize}
\end{question}

\begin{question}\label{q:volume_unfaithful}
    Is it possible to lower bound the volume of unfaithful distributions for a set of probability distributions $\mathcal{P}_G$ with parametrisation $\mathbf{a}_G$ that are Markovian to an arbitrary graph $G$? \cite{uhler_geometry_2013}
\end{question}

\begin{question}
    Can entropy-based constraints simplify the analysis of sets of faithful distributions in Question \ref{q:volume_unfaithful} (I don't presently understand what entropy-based constraints are, so this question might make no sense. I just recall a comment that entropy based constraints can be easier to work with than geometry of algebraic varieties).
\end{question}

Both the faithfulness assumption and additive noise model causal discovery identify a "contrast function" $C(P):\mathcal{P}\to\mathbb{R}$ of a probability distribution $P$ that, given a measure $\mu_G$ over $\mathcal{P}$ for each CBN $G$, takes generic values for to some CBNs $\mathcal{G}^*$ and nongeneric values for all others $\mathcal{G}^{*C}$. That is, $\mu_{G^*}(C(P))>0$, and $\mu_{G'}(C(P))=0$ for any $G'\in\mathcal{G}^{*C}$.

\begin{question}\label{q:generalised_genericity}
    These genericity conditions hold for a general class of measures $\mu_G$ - for example, if a distribution $P$ is parametrised by $\{a_i\}$ for a graph $G$, then the genericity conditions might hold for any continuous prior over $\{a_i\}$.
    
    However, to get uniform consistency results, we need to allow for some slack around the value of $C(P)$, and consequently need to shift from "non-transparent" probability distributions having measure 0 under $\mu_G$ to having small measure under $\mu_G$.
    
    For what class of measures can we conclude from $\mu_G(C(P)=c_0)=0$ and $|C(\hat{P})-c_0|<\delta$ that $\mu_G(C(\hat{P})\in B_\delta (c_0)) < \epsilon$? Does this depend strongly on $C$?
    
    $B_\delta(x)$ is a ball of radius $\delta$ around $x$.
\end{question}
    
\begin{question}
    Are there any general properties that contrast functions must satisfy?
\end{question}

\begin{question}
    Is there a method to generate contrasts $C$? See \cite{besserve_group_2017} for a discussion of using group transformations for generating measures $\mu_G$.
\end{question}
