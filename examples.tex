\subsection{Examples}

\subsubsection{Example: Observational Causal Inference}

\begin{itemize}
    \item $\mathcal{V}=\{Q,\mathcal{Z},X,Y\}$
    \begin{itemize}
        \item Policy $Q=\{0\}\cup \mathbf{Q}$
        \begin{itemize}
            \item $Q=0$ is a special ``passive'' policy
        \end{itemize}
        \item Outcome $Y\in[0,1]$
        \item Covariates $X$ and $\mathcal{Z}$
    \end{itemize}
    \item $\mathscr{E}=\{\{1\}\newmoon \to Q, \{I,Z\}\newmoon \to X,\{I,Z,X\}\newmoon\to Y\}$
    \item The cost $\rho(P(Y=y)) = \mathbb{E}[Y]$
    \item $D\sim P^\mu(X=x,Y=y,\mathcal{Z}'=\mathbf{z}'|Q=0)$ where $\mathcal{Z}'\subset \mathcal{Z}$
\end{itemize}

Assume that $P(\mathcal{V}\setminus\{Q\}|Q=q)=P(\mathcal{V}\setminus\{Q\}|do(Q=q))$.

Assume also that we know $P(X=x|Q=q)$.

The causal structure assumed gives us 
\begin{align}
    P(Y=y|do(Q=q)) = \sum_{\{z,x\}} P(Y=y|Q=q,Z=z,X=x) P(Z=z,X=x|Q=q)
\end{align}

Suppose $Y\CI Q|Z,X$ and $Z\CI Q|\emptyset$. Then
\begin{align}
    P(Y=y|do(Q=q)) &= \sum_{\{z,x\}}P(Y=y|Z=z,X=x,Q=0) P(X=x|Q=q,Z=z) P(Z=z|Q=0)\\
                  &= \sum_{\{z,x\}}P(Y=y|Z=z,X=x) P(X=x|Q=q,Z=z) P(Z=z)
\end{align}

Suppose $Z'=Z$. Then 
\begin{align}
    P(Y=y|do(Q=q)) = \sum_{\{z,x\}}P(Y=y|Z'=z',X=x) P(X=x|Q=q,Z'=z') P(Z'=z')
\end{align}

Also if $Z'=Z$ then by the back-door criterion $P(Y=y|Z'=z',do(X=x))=P(Y=y|Z'=z',X=x)$. Thus

\begin{align}
    P(Y=y|do(Q=q)) = \sum_{\{z,x\}}P(Y=y|Z'=z',do(X=x)) P(X=x|Q=q,Z'=z') P(Z'=z')
\end{align}

Suppose also for $I\in\{1,..,N\}$ we have $P(X=x|Q=q,Z'=z')\implies P(X=x|Q=q)$ - this matches a typical account of interventions where they have a ``passive'' state ($Q=0$) when their children may have other dependencies, and an ``active'' state where other dependencies are cut off. Then $Q\neq 0\implies$

\begin{align}
    P(Y=y|do(X=x)) &= \sum_{z} P(Y=y|Z'=z',X=x) P(Z'=z') \\
    P(Y=y|do(Q=q)) &= \sum_{x} P(Y=y|do(X=x)) P(X=x|do(Q=q))
\end{align}

This consists of quantities that can be computed from the data or are assumed to be known. $P(Y=y|do(Q=0))$ is known directly from the data.

Thus, under many conditions, $P(Y=y|do(X=x))$ may be helpful in computing $P(Y=y|do(Q=q))$ even though $do(X=x)$ doesn't itself correspond to any actions/policies.

Assumptions:
\begin{enumerate}
    \item Limited direct effect of $Q$
    \begin{itemize}
        \item $Y\CI Q|Z,X$
        \item $Z\CI Q|\emptyset$
    \end{itemize}
    \item $Q\neq 0$ screens off parents
    \begin{itemize}
        \item $X\CI Z|Q\neq 0$
    \end{itemize}
    \item Full parent set of $X$ is observed
    \begin{itemize}
        \item $Z'=Z$
    \end{itemize}
\end{enumerate}

Assumption 3 is typical - it enables $P(Y|do(X))$ to be estimated from the data. Assumptions 1 and 2 I have not seen elsewhere.

\subsubsection{Example: $N$-Armed Bandit}\label{sssec:n_armed_bandit}

\begin{itemize}
    \item $\mathcal{V}=\{I,X,Q\}$
    \begin{itemize}
        \item Action $I\in\{0,1,..,N\}$
        \item Outcome $X\in[0,1]$
        \item Policy $Q$ is a parametrisation of an $N$-dimensional categorical distribution
\end{itemize}
    \item $\GS{G}=\{\mathscr{G}\}$ where $\mathscr{G}=\{\mathcal{V},\mathscr{E}\}$ and $\mathscr{E}=\{I\to X, \mathcal{U}\not\to Q,\mathcal{U}\setminus Q\not\to I\}$
    \item $\Pi=\{\pi(I=n|Q)\}$ where $\pi(I|Q)=\text{Cat}_Q(N)$
    \item The cost $\rho(P(x)) = \mathbb{E}[X]$
    \item Given optimal action $q^*$, the regret $\nu$ is $\nu(q)=\rho(P^\mu (x|q^*)) - \rho(P^\mu(x|q))$
    \item $D\sim P^\mu(x,i)$
\end{itemize}

We assume $Q$ is parentless, so we have $P^\mu(x,i|q) = P^\mu(x|i)P^\mu(i|q)$ for any $q\in \text{Range}(Q)$. Thus we can find $q^*=\argmin_q P^\mu(x,i|q)$.

Bandit problems are usually sequential and don't assume the data is given. The assumptions here make the problem somewhat trivial.


\subsubsection{Example: Randomised Experiment}\label{ssssec:randomised_expt}

Consider a simple randomised experiment with one treatment and one control condition.

\begin{itemize}
    \item $\mathcal{V}=\{I,X,Q\}$
    \begin{itemize}
        \item Policy $Q\in[0,1]$: Assign to treat with probability $Q$
        \item Intervention $I\in\{0,1\}$: 0 - control, 1 - treat
        \item Outcome $X\in\{0,1\}$: 0 - recovered, 1 - sick
\end{itemize}
    \item $\GS{G}=\{\mathscr{G}\}$ where $\mathscr{G}=\{\mathcal{V},\mathscr{E}\}$ and $\mathscr{E}=\{Q\to I, I\to X, \mathcal{U}\not\to Q, \mathcal{U}\setminus Q\not\to I\}$
    \item $\Pi=\{\pi(I=i|Q=q)\}$ where $\pi(I=1|Q=q)=q$
    \item The cost is the expected rate of recovery $\rho(P(x,i)) = \mathbb{E}[X]$
    \item Given optimal policy $q^*$, the regret $\nu$ is $\nu(q)=\rho(P^\mu (x,i|q^*)) - \rho(P^\mu(x,i|q))$
    \item $D\sim P^\mu(x,i)$
\end{itemize}

The edge assumptions imply $P^\mu(x,i|q) = P^\mu(x|i)P^\mu(i|q)$ for any $q$. Thus we can compute $P^\mu(x,i|q)$ for any $q$ given estimates of $P^\mu(x|i)$.

Note that the potential outcomes quantity of average treatment effect (ATE) is $\rho_Q(1)-\rho_Q(0)$, which if it is positive is equal to the regret $\nu(0)$.

The edge assumptions made here may not always hold. Consider ``performance bias'', one of the biases identified in the Cochrane Reviewer's Handbook\cite{collaboration_cochrane_nodate}. Performance bias occurs when different standards of care are applied to the treatment and control group. 

We can imagine an experiment where the treatment group receives unusually good care as a result of the treatment assignment, and not as a result of receiving the treatment itself. The problem arises if we cannot expect this better treatment to occur \emph{outside} of the experiment, leading to different treatment effects under experimental and nonexperimental conditions.

We could consider this a violation of policy exchangeability in the following sense: under experimental conditions, we use the policy $Q=0.5$, while on the basis of experimental results we use either $Q=0$ or $Q=1$. Because $Q$ will be correlated with the presence of absence of experimental conditions, which themselves affect the outcomes, performance bias can lead to a situation where $P(X|I=1,Q=0.5)\neq P(Q|I=1,Q=1)$. This is a violation of policy exchangeability, which cannot occur if $Q$ were truly parentless.

This approach could be made to closely mirror the potential outcomes approach if we formulated it in such a way that $Q$ was randomised rather than randomisation being introduced in the kernel $Q\to I$. This justifies the parentlessness of $Q$, but $Q$ is not randomly chosen in nonexperimental settings and so the potential outcomes approach needs some additional assumption to justify using its results in a nonexperimental setting.


\subsection{Propensity score matching \& weighting}

Propensity score matching is a technique for 


\subsubsection{Example: Genetics/Hidden confounding}

Based on \cite{ranganath_multiple_2018}.

\begin{itemize}
    \item $\mathcal{V}=\langle \mathcal{X}_{g}, \mathcal{X}_{p}, \mathcal{X}_{c},\mathcal{J}\rangle$
    \begin{itemize}
        \item $\mathcal{X}_g$ genetic variables
        \item $\mathcal{X}_p$ phenotypic variables
        \item $\mathcal{X}_c$ hidden confounders
        \item For each $X_g^i\in \mathcal{X}_g$ there is a perfect intervention $J^i\in\mathcal{J}$
    \end{itemize}
  \item $\GS{G}=\{\mathcal{V},\GS{E}\}$ with $\GS{E}=\{J^i\to X_g^i|J^i\in \mathcal{J}\}\cup \{Q\Rightarrow\mathcal{J}, \mathcal{X}_c\Rightarrow \mathcal{X}_g, \mathcal{X}_g\not\rightarrow \mathcal{X}_g, \mathcal{X}_c\rightarrow \mathcal{X}_g, \mathcal{X}_g\rightarrow \mathcal{X}_p, \mathcal{U} \not\to \mathcal{J}\}$
  \begin{itemize}
      \item There is an intervention for each genetic variable
      \item All genetic variables share confounders (this is the first key assumption,  ``shared confounding'')
      \item Genetic variables are independent given confounders (this is the second key assumption of the paper)
      \item Confounders may influence the phenotype, not the other way around
      \item Genes may influence phenotype, not the other way around
      \item $\mathcal{J}$ are perfect interventions
  \end{itemize}
  \item $\Pi=\{\pi(X_g|J)|J\in \mathcal{J} \}$ where $\pi(X^j_g|J^j=(1,j^i))=\delta_{x^i_g j^i}$
  \item $\lambda:\Delta(\mathcal{X}_p,\mathcal{J})\to\mathbb{R}$ is some sort of prediction loss
  \item $D\sim P^\mu(\mathcal{X}_p,\mathcal{X}_g)$
\end{itemize}


By standard causal reasoning, $P(\mathcal{X}_p|\mathcal{X}_g=\mathbf{x}_g,X_c=x_c) = P(\mathcal{X}_p|\mathcal{J}=(1,\mathbf{x}_g),X_c=x_c)$.

The problem is then one of estimating the unmeasured $X_c$. Under the assumptions made here, we have $i\neq j\implies X^i_g \CI X^j_g | X_c$. 

The authors of \cite{ranganath_multiple_2018} use this fact to build an objective of the form
\begin{align}
    \lambda(\theta,\beta)=\mathbb{E}_{p_\theta(X_c)}[p_\beta (\mathcal{X}_g|X_c)]-\alpha \mathbb{I}_\theta (X_g^i,X_c|\mathcal{X}_g\setminus X_g^i)
\end{align}

Where $\mathbb{I}_\theta$ is mutual information. This finds a confounder $X_c$ that predicts $\mathcal{X}_g$ well subject to the constraint that $X^i_g \CI X_c | \mathcal{X}_g\setminus X^i_g$ in the limit of $\mathcal{X}_g|\to \infty$. 

In practice, limited data and gene loci make this problem more difficult.

\subsubsection{Example: Observational Causal Structure Inference}

Observational causal inference is sometimes cast as the problem of identifying the correct causal graph $\mathscr{G}^*$ from observations of $D\sim P^\mu(\mathcal{X})$. To me it makes more sense to aim for the distribution $P^\mu(\mathcal{X},\mathcal{I})$.

A typical observational causal inference problem:

\begin{itemize}
    \item $\mathcal{V}=\{\mathcal{J},\mathcal{X}\}$
    \item $\GS{G}$ and $\Pi$ such that $\mathcal{J}$ are perfect interventions
    \item Loss $\lambda:\mathscr{G}(\mathcal{V})\to\mathbb{R}$ that penalises:
    \begin{itemize}
        \item Non-markovianity of $\mu(\mathcal{J},\mathcal{X})$ with respect to $\mathcal{G}$
        \item Non-minimality of $\mathscr{G}$ with respect to $\mu$
    \end{itemize}
    \item $D\sim P^\mu(\mathcal{A})$ where $\mathcal{A}\subset\mathcal{X}$
\end{itemize}

The above problem is ideally answered with $\GS{G}^*$, a set of minimal graphs with respect to which $\mu$ is Markovian.

Strong assumptions are typically made in order to make progress on this problem. One example is the pair of assumptions $\mathcal{A}=\mathcal{X}$ (no hidden variables) and 
\emph{faithfulness} (see Definition \ref{def:faithfulness}).

\begin{remark}
Is faithfulness equivalent to assuming that if $\mathcal{X}_i\CI \mathcal{X}_j | \mathcal{X}_k$ in $P^\mu(\mathcal{X})$, then $\mathcal{X}_i\CI \mathcal{X}_j | \mathcal{X}_k$ in $P^\mu(\mathcal{X}|\mathcal{I}=\mathbf{i})$ for all $\mathbf{i}\in\mathcal{I}$? 
\end{remark}

\begin{remark}
If we are ultimately interested in inferring $\mu(\mathcal{X},\mathcal{J})$ with perfect interventions $\mathcal{J}$, is \emph{minimality} necessary? It may be sufficient to have a graph to which $\mu$ is Markovian.
\end{remark}

\subsubsection{Example: Observational Structure Inference with Soft Interventions}

If we augment the dataset with observations under soft interventions, the structural inference problem becomes more tractable.

\begin{itemize}
    \item $\mathcal{V}=\{\mathcal{I},\mathcal{J},\mathcal{X}\}$
    \item $\GS{G}$ and $\Pi$ such that $\mathcal{J}$ are perfect interventions and $\mathcal{I}$ are soft interventions
    \item Loss $\lambda:\mathscr{G}(\mathcal{V})\to\mathbb{R}$ that penalises:
    \begin{itemize}
        \item Non-markovianity of $\mu(\mathcal{J},\mathcal{X})$ with respect to $\mathcal{G}$
        \item Non-minimality of $\mathscr{G}$ with respect to $\mu$
    \end{itemize}
    \item $D\sim P^\mu(\mathcal{X},\mathcal{I})$
\end{itemize}

It has been shown by \cite{yang_characterizing_2018} that assuming there is for every $J\in \mathcal{J}$ an $I\in \mathcal{I}$ with the same set of edges, it is possible to find the Markov Equivalence Class of $P^\mu(\mathcal{X},\mathcal{J})$ from $D\sim P^\mu(\mathcal{X},\mathcal{I})$.


\subsubsection{Example: Factorised Action Set}

\begin{itemize}
    \item $\mathcal{V}=\mathcal{Q}\cup \mathcal{X}\cup\mathcal{U}$
    \begin{itemize}
        \item $\mathcal{Q}=\{S,A\}$
        \item $\mathcal{X}=\{F,R\}$
    \end{itemize}
    \item Cost $C(P(\mathcal{X},\mathcal{I}))=\mathbb{E}[R]$
    \item $P(\mathcal{X}|\mathcal{Q})=P(\mathcal{X}|do(\mathcal{Q}))$
\end{itemize}

Suppose $S$ enumerates a number of possible implementation strategies an education department could take for literacy training (for example, $S=0$ is ``grant schools money for literacy training'', $S=1$ is ``mandate schools send teachers to departmentally provided literacy training'' etc.) and $A$ enumerates a number of literacy teaching approaches ($A=0$ is ``systematic phonics'', $A=1$ is ``whole language'' etc.). A literacy strategy is a pair $Q=(S,A)$.

$F\in [0,1]$ is an indication of ``implementation fidelity'' and $R$ is the result of a literacy test with binary pass/fail outcome.

If $|\text{Range}(S)|=N$ and $|\text{Range}(A)|=M$ then $|\text{Range}(Q)|=NM$. Thus in general $P(R=1|Q)$ has $MN$ parameters.

Suppose $P(R|F,A)$ has the following form for all $i\in \text{Range}(A)$:
\begin{align}
    P(R=1|F=f,A=i) = \alpha_i f
\end{align}
$P(R|F,A)$ then has $M+1$ parameters.

Suppose $P(F|S=i) = \mathcal{N}(\mu_i,\sigma_i)$.

Suppose that $P(R|F,A)=P(R|F,A,S)$ and $P(F|S)=P(F|A,S)$. Then
\begin{align}
    P(R|Q) &= \int P(R|F=f,A,S)P(F=f|A,S) df \\
           &= \int P(R|F=f,A)P(F=f|S) df \\
    \mathbb{E}(R|S=i,A=j) &= \alpha_j \mu_i
\end{align}
Given these assumptions, $\mathbb{E}[R|Q]$ can be modeled with just $N+M$ parameters. 

Under the assumptions made so far, $A$ and $S$ look similar to soft interventions on $R$ and $F$ respectively. In this light, the assumption $P(R|F,A)=P(R|F,A,S)$ has the flavour of assuming $P(R|F,A)=P(R|do(F),A)$. However, the only graphical assumption that has been made here is $\{1\}\to \mathcal{Q}$, which isn't sufficient either to classify $A$ and $S$ as soft interventions or to devine $P(R|do(F),A)$.

Causal reasoning is sometimes framed in terms of ``reusable modules'' \cite{pearl_causality:_2009} \cite{dawid_beware_2010}. Here $P(R|F,A)$ and $P(F|S)$ are two such modules, held to be invariant to the different manipulations $S$ and $A$ respectively. Note that $P(R|S,A)$ has the same dimensionality as $P(R|F,A)$ but under our assumptions the latter has a simpler form - here simpler in the naive sense of ``number of parameters''.

Many extant problems in causal reasoning focus on determining the structure or part of the structure of one ``module''. This example focuses on where there might be advantages obtained from reasoning in terms of modules rather than going straight for $P(\mathcal{X}|\mathcal{Q})$. 

A relevant piece of recent work is the ``AI Physicist'', which predicts $y_{t}$ from $x_t=(y_{t-T},...,y_{t-1})$ by learning a set of simple theories $\mathcal{T}=(\mathbf{f},c)$ where $\mathbf{f}$ predicts $y_t$ from $x_t$ and $c$ classifies $x_t$ as inside or outside the theory's domain of validity. Conceptually, each $\mathbf{f}$ seems similar to a module that is invariant within some set of contexts given by $c$. It is less clear how invariance withing a context relates to invariance with respect to certain manipulations.

% $\mathscr{M}$ is the collection of kernels $\theta,\phi,\mu$. $Q$ is a sub-$\sigma$-algebra of $\mathcal{H}$ generated by $\phi$; i.e. it is the coarsest $\sigma$-algebra on $H$ that distinguishes every possible policy $\phi(\cdot,q,\cdot)$.

% \begin{definition}[Counterfactual optimality]
% The $P_H$,$\mathscr{M}$-counterfactual optimal policy given some cost $C:\Delta(I,V)\to \mathbb{R}$ and $P(i,v,a)$ is $\underline{\phi}^* = \argmin_\phi C(\sum_a P(v|i,a)\phi(a,i)P(a))$.
% \end{definition}

% \begin{definition}[Conditional optimality]
% The conditionally optimal sub-history $q^*$ given some cost $C:\Delta(I,V)\to \mathbb{R}$ is $q^*=\argmin_q C(P(i,v|q))$.
% \end{definition}

% \begin{definition}[$\mathbf{h}_0$-optimizability]
% A causal model $P_H$, $\mathscr{M}$ with $\mathbf{h}_0\in\mathcal{H}$ is $\mathbf{h}_0$-optimizable if the counterfactual and conditional optimality conditions agree:
% \begin{align}
%     \underline{\phi}^* &= \phi(\cdot,q^*,\cdot)\\
% \end{align}
% where $q^*$ is the conditionally optimal sub-history and $\underline{\phi}^*$ is the $P_H(h|\mathbf{h}_0)$, $\mathscr{M}$-counterfactually optimal policy
% \begin{align}
%     q^* &= \argmin_q C(\sum_a P(v|i,a,q)\phi(a,q,i)P(a|q))\\
%     \underline{\phi}^* &= \argmin_{\underline{\phi}} C(\sum_a P(v|i,a,\mathbf{h}_0)\underline{\phi}(a,i)P(a|\mathbf{h}_0))
% \end{align}
% \end{definition}

% \begin{definition}[$\mathbf{h}_0$-optimizability]
% A causal model $P_H$, $\mathscr{M}$ with $\mathbf{h}_0\in\mathcal{H}$ is $\mathbf{h}_0$-identifiable if it is $P_\phi$-identifiable given the estimator
% \begin{align}
%     P_\phi(i,v) = \sum_a P(v|i,a,\mathbf{h}_0) \phi(a,i) P(a|\mathbf{h}_0)
% \end{align}
% \end{definition}

% \begin{example}[Useless medicine]
% $S\in \{0,1\}$ is information on whether or not someone is sick, $I=\{0,1\}$ is a variable representing whether or not they received treatment and $V=\{0,1\}$ is a variable representing whether or not they recovered. The experimenter is aiming (perhaps unwisely) to maximise recovery, while avoiding unnecessary treatment: $C(P(i,v))=-\mathbb{E}[V-0.5I]$. 

% Suppose that the treatment does nothing, but sick people always recover anyway: $P(v|i,s) = P(v|s) = \delta_{vs}$.

% The causal model consists of the distribution $P_S(s)$ and the composition of Markov kernels $\theta,\pi,\phi$ and $\mu$. The history $H$ will consist of two parts: $S$, whether or not a patient is sick and $L\in\{0,1\}$ which is whether or not the treatment decisions are being made in the ``learning'' phase. Thus $H=S\times L$, and $\mu(i,[s,l],v)=\delta_{vs}$. Define $\mathbf{h}_0\in\mathcal{H}=\{l=1\}=\{(s,l)|l=1\}$.

% First, we'll consider a version of the problem that is not $\mathbf{h}_0$-identifiable. Take $A=\varnothing$, $Q=[0,1]$ and $\phi(q,i)=\text{Bernoulli}(q)$. In the learning phase, suppose $P(q|s,l=1)=\delta(q-s)$. That is treatment is given with probability $q$ and during the learning phase $\pi$ people are treated iff they are sick.

% In this case, $p(v|i=1,q=1,l=1)=\delta_{v1}$ while $p(v|i=1,q=0,l=0)=\delta_{v0}$ (where conditioning on the impossible $i=1,q=0$ is defined by adding independent noise to $\phi$ and taking the limit as that noise goes to 0), so policy exchangeability does not hold. Domain stability trivially holds, however, as $A=\varnothing$.

% To see that the problem is not identifiable, note that $p(v|i=1,l=1)=\delta_{v1}$ while $p(v|i=0,l=1)=\delta_{v0}$. Thus if we were to estimate $P(i,v|q=0,l=0)$ we would calculate

% \begin{align}
%     \tilde{P}(i,v|q=0,l=0)&=P(v|i,l=1) \phi(0,i) \\
%     \tilde{P}(i,v|q=0)&=\delta_{v0}
% \end{align}

% While in fact

% \begin{align}
%     P(i,v|q=0,l=0)&=
% \end{align}

% Enumerate possible treatment policies with $Q\in \{0,1\}\times[0,1]$ where 
% $$\phi(a,[q_0,\epsilon],i) = (1-\epsilon)\left[q_0\delta_{ai} + (1-q_0)(1-\delta_{ai})\right] + \epsilon$$
% $q_0\in \{0,1\}$ indicates whether sick or healthy people are more likely to be treated and $\epsilon$ is the degree to which treatment is randomised.



% Suppose furthermore that only sick people are treated. That is, given $Q=\{0,1\}$ and $\phi$ such that $\phi(\_,q,i)=\delta_{qi}$, we have $\pi(q,s)=\delta_{qs}$. We take $A=\emptyset$

% The above assumptions lead straightforwardly to $P(v|i)=\delta_{vi}$, a typical case of ``correlation is not causation''. Recovery is perfectly matched with treatment, but this is contingent on the unwise choice of policy in which only sick people were treated. If people were treated randomly we would find that recovery was independent of treatment.

% Returning to our cost function, it would naively appear that the policy $\phi_{q1}:A\mapsto \delta_{i1}$ is optimal:

% \begin{align}
%     C(P(v|i)\delta_{i1}) &= 0.5\\
%     C(P(v|i)\delta_{i0}) &= 0
% \end{align}

% However, if we adjust for $S$ we find that we should not treat. $P(v|i,q,s)=P(v|s)$ and so $P(i,v|q,s)=P(v|s)P(i|q)$ and

% \begin{align}
%     C(P(v|s)P(i|q)) = 0.5q - s
% \end{align}

% Here, the problem was that the set $A=\emptyset$ was not large enough. If $S$ were in $\mathcal{A}$ we could have correctly concluded that sickness and not treatment was driving recovery.
%\end{example}