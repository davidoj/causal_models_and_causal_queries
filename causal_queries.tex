\section{Causal Policies}

A proposed general framework for decision-theoretic causal queries:

% \begin{definition}[Causal model query]\label{def:causal_query_1}
% Given complete dataset $D$ over domain $V$ and a sample $S\subset D$, a class of causal models $\mathcal{M}$ and a $D$-dependent loss $\mathcal{L}_D:\mathcal{M}\to \mathbb{R}$, using the sample $S$ find a model $M\in \mathcal{M}$ minimising $\mathcal{L}_D$?
% \end{definition}

% Thinking aloud: given a distinction between intervention and other variables $V=X\times \mathcal{I}$, the causal model defined in \ref{def:causal_models} explicitly avoids requiring a marginal distribution $P(\mathcal{I})$, though the conditional $P(X|\mathcal{I}=I)=M(I)$ exists.

% On the other hand, given a complete dataset $D$, $P(\mathcal{I})$ will exist and $\mathcal{L}_D$ will depend on it. It seems undesirable for a causal query to depend on the selection of intervention.

% Try stepping up a level of generality:

% Let $\mathcal{I}$ be an arbitrary set of interventions. If we imagine a robot interacting with the world, we can think of $\mathcal{I}$ as the set of all possible signals the robot could send to its actuators. We will later discuss when it makes sense to consider particular types of intervention such as $do$-type interventions.

Define the collection of conditionals $P(X|\mathcal{I})=\{P(X|I)|I\in\mathcal{I}\}$.

\begin{definition}[Causal policy]\label{def:causal_query_2}
Given a set of conditionals $P(X|\mathcal{I})$ with domain $X$ and interventions $\mathcal{I}$ and a cost function $C:X\times\mathcal{I}\to\mathbb{R}$ find $I$ minimising $\mathbb{E}_{P(X|I)}[C(X,I)]$. The resulting $I$ is the \emph{causal policy}
\end{definition}

\subsection{Problems with causal policies}\label{ssec:prob_causal_policy}

A causal policy (Def \ref{def:causal_query_2}) suggests the existence of an agent with an ability to conduct the interventions $\mathcal{I}$. However, definitions so far leave open the mechanism by which the agent can conduct such interventions. This is undesirable, as if the agent implements a policy using some mechanism $\mu_I$ to conduct intervention $I$, we might justifiably be concerned about whether $P(X|I,\mu_I)=P(X|I)$. This isn't just a theoretical concern - it is easy to come up with examples of situations where $P(X|I,do(I))\neq P(X|I,\neg do(I))$. If we add a node representing $\mu_I$ to the model, we might shift our concern to a potential metamechanism $\mu'_I$ and so forth. It might be intuitively obvious in many cases that we can stop when we reach a given level, but the lack of a formal account is unsatisfying.

\begin{definition}[Causal optimization problem]\label{def:copt_prob}
A causal optimization problem is a tuple $\langle \mathcal{I}, V, \mathcal{F}, m, C\rangle$. $V$ is a set of variables and $\mathcal{I}$ is a set of interventions and $V$ is a set of random variables. $\mathcal{F}$ is a set of functions $V\to \mathcal{I}$ and $m$ is a map $\mathcal{F}\to \mathcal{P}_{V\times \mathcal{I}}$, where $\mathcal{P}_{V\times \mathcal{I}}$ is a set of probability distributions on $V\times \mathcal{I}$. $C:V\times \mathcal{I} \to \mathbb{R}$ is a cost function.
\end{definition}

\subsection{Functional policy}

My proposed solution is to incorporate a policy function into the world model. This demystifies what exactly "interventions" are - they are policy function outputs.

\begin{definition}[Functional policy]\label{def:fp_cpq}
Suppose we have a domain $X\times\mathcal{I}$, a set of candidate policies $\mathcal{F}:\{X\to \mathcal{I}\}$, a map $m:\mathcal{F}\to \mathcal{P}$ where $\mathcal{P}$ is the space of probability distributions over $X\times\mathcal{I}$ and a cost function $C:X\times\mathcal{I}\to \mathbb{R}$.

The functional policy is
\begin{align}
    \Pi=\mathrm{argmin}_{f\in\mathcal{F}}\mathbb{E}_{m(f)}[C(x,I)]    \label{eq:func_cp}
\end{align}
\end{definition}

As the codomain of the map $m$ is a space of probability distributions, I will use the notation
\begin{align}
    P_\Pi := m(\Pi)
\end{align}

Needs work: I think that the domain of functions in $\mathcal{F}$ should probably be restricted to things that happen before the function is computed. I'm reluctant to include time if it can be avoided, so leaving it as a note for now.

Definition \ref{def:fp_cpq} is motivated by the following consideration: Suppose we have a true set of conditionals $P^*(X|\mathcal{I})=\{P^*(X|I):\forall I \in \mathcal{I}\}$. Deducing a policy from \ref{def:causal_query_2} using the model $P(X|\mathcal{I})=P^*(X|\mathcal{I})$ is not sufficient to ensure the chosen policy is actually optimal because we haven't specified a model of how the agent interacts with $\mathcal{I}$. Suppose, instead, we have a map $m^*:\mathcal{F}\to\mathcal{P}$ that is true for all $f\in\mathcal{F}$ and derive an optimal policy $\Pi\in\mathcal{F}$ following \ref{def:fp_cpq} with $m=m^*$. This is sufficient to ensure that $\Pi$ is superior to all other policies in $\mathcal{F}$.

% \begin{remark}
% If the set of distributions $\mathcal{P}$ are able to be represented by structural equation models, all variables are functions of other variables in the model. Intervention variables are then distinguished only by the freedom to choose which function computes them.
% \end{remark}


\subsubsection{Self-optimisation}\label{sssec:self_reflection}

Finding an optimal policy via Definition \ref{def:fp_cpq} is itself the application of a map $Q:\{\mathcal{F}\}\times\{\mathcal{F}\to\mathcal{P}\}\times\{\mathcal{I}\to\mathbb{R}\}\to\{X\to\mathcal{I}\}$ such that $Q(\mathcal{F},m,C)=\Pi$ (with symbols defined as above). 

Introducing a map $G\{X\to X\to \mathcal{I}\} \to \{X\to\mathcal{I}\}$, $G(Q)$ is itself a policy - in fact it is the policy actually implemented by the agent, while the idealised $\Pi:=Q(X)$ will, in general, forget some of the inputs of $G(Q)$. 

It is interesting to ask whether it is possible to set the optimisation problem up so that $\Pi=G(Q)$, and what implications this might have. To do so, we must ensure that $G(Q)\in\mathcal{F}$, and so we must augment the domain $X$ with variables representing $\mathcal{F}$,$m$ and $C$.

Needs work: the fact that the model $m$ must contain a representation of itself is suggestive of an incomputability result. Is there such a result here?

Needs work: Generally, when is it possible to take a policy $\Pi_1:X\to\mathcal{I}$ and produce a simplified policy $\Pi_2:C\to\mathcal{I}$ where $C\subset X$ which achieves the same cost?


\subsection{Functional policies in Bayesian Networks}

The definitions above do not require the probability distributions $\mathcal{P}$ be Markovian. However, Markovian distributions permit easy representation of conditional independence with graphical models, so they make a good basis from which to generate examples.

We will follow the custom of labeling nodes in a graphical model with the output of the function represented by the node, so the policy node will be labeled $I$.

We will also neglect any subtleties raised by section \ref{sssec:self_reflection}.

Needs work: the inclusion of a policy function suggests that $I$ may be locally Markovian with respect to the inputs to the policy function. Need to check the exact characteristics WRT Markovianity.

\subsubsection{Simple case}\label{sssec:simple_case}

Let $\mathcal{I},X=\{0,1\}$ and $C(x,I)=x$.

\begin{center}
\begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
semithick ,
state/.style ={ circle ,top color =white ,
draw , text=blue , minimum width =1 cm}]
\node[state] (A) [left] {$I$};
\node[state] (B) [right = of A] {$X$};
\path (A) edge [] node[above] {} (B);
\end{tikzpicture}
\end{center}

We will partially define the probability distribution with the following structural equation:
\begin{align*}
    X:=I
\end{align*}
Choosing a function that computes $I$ will complete the definition.

Given the dependences indicated, we have as $\mathcal{F}$ the set of constant functions on $\mathcal{I}$. Of these, $I\mapsto0$ minimises $C$.

In this simple case, finding the functional policy reduces to finding the best intervention. 

\subsubsection{Additional dependence}

Let $\mathcal{I},X_1,X_2=\{0,1\}$ and $C(x_1,x_2,I)=x_1$. Suppose $X_2$ is observed before $I$ is computed, so it is a potential input of the policy.

\begin{center}
\begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
semithick ,
state/.style ={ circle ,top color =white ,
draw , text=blue , minimum width =1 cm}]
\node[state] (B) [right] {$X_1$};
\node[state] (A) [above left = 1cm and 3cm of B] {$X_2$};
\node[state] (C) [below left = 1cm and 3cm of B] {$I$};
\path (A) edge [] node[above] {} (B);
\path (C) edge [] node[above] {} (B);
\draw[dashed] (A) -- (C);
\end{tikzpicture}
\end{center}

We will partially define the distribution with:
\begin{align*}
    X_1 := \mathrm{XOR}(I,X_2)
\end{align*}

We could consider policies that are constant on $\{0,1\}$, or maps $\{0,1\}\to\{0,1\}$. Clearly in this case we want $\Pi:I\mapsto X_2$. If we further knew that $P(X_2=0)=1$, then the constant policy $f:I\mapsto 0$ would perform equally well.

\begin{remark}
The optimal policy isn't straightforward to derive using Causal Bayesian Networks. If we consider the setup above where we are permitted to conduct interventions on the node labeled by $I$, under the standard definition of intervention we would assume that it cuts off any dependence of $I$ on $X_2$. This would limit us to what are here described as constant policies, which are not in general optimal. 

We could address this by proposing that $do()$-interventions could depend on the value of $X_2$ even if $I$ does not depend on it directly. This would invalidate some properties of Causal Bayesian Networks.
\end{remark}

\begin{remark}
If the policy depends on $X_2$, we never need to know the full probability distribution to find the optimal policy.
\end{remark}

\subsubsection{Hidden dependence}
Let $\mathcal{I},X_1,X_2=\{0,1\}$ and $C(x_1,x_2,I)=x_1$. Suppose $X_2$ is not observed.

\begin{center}
\begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
semithick ,
state/.style ={ circle ,top color =white ,
draw , text=blue , minimum width =1 cm}]
\node[state] (B) [right] {$X_1$};
\node[state,dashed] (A) [above left = 1cm and 3cm of B] {$X_2$};
\node[state] (C) [below left = 1cm and 3cm of B] {$I$};
\path (C) edge [] node[above] {} (B);
\draw[dashed] (A) -- (B);
\end{tikzpicture}
\end{center}

We will assume the following structural equations
\begin{align*}
    X_2 &:= \mu_{X_2}\\
    X_1 &:= \mathrm{XOR}(I,X_2)
\end{align*}
Where $\mu_{X_2}\sim \mathrm{Bernoulli}(0.2)$.

In this case, the problem appears to be identical to a problem with the dependence structure of \ref{sssec:simple_case}, and the optimal policy is constant $\Pi:I\mapsto 1$.

\subsubsection{Confounded observations}
Let $\mathcal{I},X_2=\{0,1\}$, $X_1=\{0,1,2\}$ and $C(x_1,x_2,I)=x_1$. Suppose $X_2$ is not observed.

Suppose also that we make infinite observations under some fixed policy $f$, and we are then free to impose our preferred policy $\Pi$.

\begin{center}
\begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
semithick ,
state/.style ={ circle ,top color =white ,
draw , text=blue , minimum width =1 cm}]
\node[state] (B) [right] {$X_1$};
\node[state,dashed] (A) [above left = 1cm and 3cm of B] {$X_2$};
\node[state] (C) [below left = 1cm and 3cm of B] {$I$};
\path (C) edge [] node[above] {} (B);
\draw[dashed] (A) -- (B);
\draw (A) -- (C);
\end{tikzpicture}
\end{center}

We will assume the following structural equations
\begin{align*}
    X_2 &:= \mu_{X_2}\\
    X_1 &:= \neg I + X_2 \\
    f   &:= X_2
\end{align*}
Where $\mu_{X_2}\sim \mathrm{Bernoulli}(0.2)$.

The optimal policy will be $\Pi:I\mapsto 0$, but from our observations it will appear that $X_1 \CI I$ which would suggest that $\Pi$ is no better than the alternative.

\subsubsection{Randomisation}
Let $\mathcal{I}\{0,1\}$, $X_1=\{0,1,2\}$, $N=\{0,1,..,i,..,n\}$ and $C(x_1,i,I)=x_1$. Suppose $X_2$ is not observed.

We wish to estimate the distribution of $X_1$ under fixed policies $f_0:I\mapsto 0$ and $f_1:I\mapsto 1$, and we can sample the system $n$ times. In order to do this, we will in fact be sampling under the policy $r:N\mapsto \{0,1\}$.

\begin{center}
\begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
semithick ,
state/.style ={ circle ,top color =white ,
draw , text=blue , minimum width =1 cm}]
\node[state] (B) [right] {$X_1$};
\node[state,dashed] (A) [above left = 1cm and 3cm of B] {$N$};
\node[state] (C) [below left = 1cm and 3cm of B] {$I$};
\path (C) edge [] node[above] {} (B);
\draw[dashed] (A) -- (B);
\draw (A) -- (C);
\end{tikzpicture}
\end{center}

The dependence structure is identical to the previous section, which showed that in general it isn't possible to deduce the optimal policy from these observations. The difference is that we are free to choose $r$.

Suppose there is some set of ``plausible'' functions $\mathcal{H}=\{N\times I\to X_1\}$. If we choose $f'$ such that 
\begin{align}
    \max_{h\in\mathcal{H}} \left| \langle \{h(i,0):r(i)=0\}\rangle - \langle \{h(i,0):i\in\{n\}\} \rangle \right| < \epsilon \label{eq:bin_random}
\end{align}
Where $\langle\cdot\rangle$ denotes an average. If we suppose this also holds for $h(\cdot,1)$, then we can be confident that the subset of samples for which $r(i)=0$ will provide a good estimate for the distribution of $X_1$ under $f_0$, and the subset for which $r(i)=1$ will provide a good estimate for the distribution under $f_1$.

\subsection{Identifiability}

In the following discussion, define constant policies $\Pi_i:I\mapsto i$ and an arbitrary "observational" policy $\Pi_o$.

In the Pearlean framework, the causal effect of $X$ on $Y$ is identifiable if $P_{\Pi_i}(y)=P(y|do(X=i))$ is able to be consistently estimated from infinite observational data for all $i$. Given a graph $G$ over vertices $V$ representing a CBN \ref{def:CBN} and a subset $X\subset V$, denote the graph deleting all outgoing edges of $X$ by $G_{\underline{X}}$ and the graph obtained by deleting all incoming edges to $X$ as $G_{\overline{X}}$.

Under these assumptions, four conditions for the identifiability of $X$ on $Y$ are given that are claimed to be both necessary and sufficient. The faithfulness assumption is not mentioned in the reference, but I can provide an example of an identifiable effect where $P$ is minimial but not faithful to $G$ and the following conditions do not hold \cite{galles_testing_2013}. Recall definition \ref{def:dsep} for the points below:

\begin{enumerate}
    \item There is no path from $X$ to $Y$ in $G$
    \item (No back-door path) $X\perp_{G_{\underline{X}}} Y$
    \item (All back-door paths blocked) If $Z\subset V$ are observed, then $X\perp_{G_{\underline{X}}} Y|Z$
    \item (``Front-door'' criterion) There exists $Z_1, Z_2 \subset V$ such that
    \begin{itemize}
        \item $Z_2\cap X =\varnothing$
        \item $Z_1$ blocks all directed paths from $X$ to $Y$: $X \perp_{G_{\overline{X}}} Y | Z_1$
        \item $Z_2$ blocks all back-door paths between $Z_1$ and $Y$ in $G_{\overline{X}}$: $Z_1 \perp_{G_{{\overline{X}}{\underline{Z_1}}}} Y | Z_2$
        \item $Z_2$ blocks all back-door paths between $X$ and $Z_1$: $Z_1 \perp_{G_{{\underline{X}}}} X | Z_2$
    \end{itemize}
\end{enumerate}

Recall that $d$-separation statements imply conditional independences in the associated probability distribution $P_\cdot$.

The first three are sufficient for estimating $P_{\Pi_i}(y)$ from $P(y|I,z)$, while the fourth has a more complicated estimation procedure.

In the Rubin Causal Model, the set of interventions considered is two-valued: $\mathcal{I}=\{0,1\}$. Rubin uses notation that considers finite sample effects and adds assumptions to ensure that we can derive IID samples from $P_{\Pi_0}$ and $P_{\Pi_1}$, which we will neglect. Rubin shows that the population average treatment effect $\mathbb{E}_{P_{\Pi_1}} [Y] - \mathbb{E}_{P_{\Pi_0}} [Y]$ can be calculated from samples from $P_{\Pi_o}(Y|I=0)$ and $P_{\Pi_o}(Y|I=1)$ provided \emph{ignorability} holds \cite{rubin_causal_2005}.

To define ignorability properly, I'd need to define counterfactual quantities. For the moment I will use the following definition that I believe captures the same idea
\begin{align}
    P_{\Pi_o}(Y|Z,I=0) &= P_{\Pi_0}(Y|Z,I=0) \\
    P_{\Pi_o}(Y|Z,I=1) &= P_{\Pi_1}(Y|Z,I=1)
\end{align}

Needs work: the difficulty is that counterfactuals posit a "true" value of $Y(1)$ - the outcome given the treatment. It seems \emph{reasonable} to interpret this as the value of $Y$ under a constant policy of $I\mapsto 1$, but it's not obvious that this is the only possible interpretation. The definition here does at least match Pearl's definition of counterfactuals.

Pearl's identifiability condition is stronger than Rubin's. If we know $P_{\Pi_i}(y)$ for all $i\in \mathcal{I}$, then provided $P_{\Pi_i}$ has an expected value, we can trivially find $\mathbb{E}_{P_{\Pi_i}} [Y] - \mathbb{E}_{P_{\Pi_j}} [Y]$ for any $i,j\in\mathbb{R}$.

Neither set of assumptions is strictly stronger than the other. (1)-(3) of the do-calculus identifiability imply ignorability, but (4) may hold when ignorability does not. On the other hand, given a graph where an unobserved variable $U$ is a parent of both $X$ and $Y$ (and hence violating the back-door criterion), we can propose a compatible probability distribution $P$ where ignorability holds due to intransitivity unfaithfulness at $U$ \ref{def:intrans_unfaith}.

\textbf{Conjecture:} Rewriting (1)-(4) of the do-calculus conditions in terms of conditional independences rather than d-separation would render them necessary and sufficient to identify a causal effect.

\subsubsection{Relationship between causal identification and cost minimisation}

Both criteria for causal identification above consider only constant policies. Some questions arise:
\begin{itemize}
    \item Is causal identifiability along with the assumption that there is an optimal constant policy enough to enable the optimal policy to be found?
    \item Is causal identifiability (or a modification of it) enough to enable an optimal non-constant policy to be found?
\end{itemize}

\subsubsection{Identifiability and constant policy cost minimisation}

Suppose we have a causal optimization problem $\langle \mathcal{I}, V, \mathcal{F}, P_f, C\rangle$ where $\mathcal{F}$ is the set of constant functions $\Pi_i$ on $\mathcal{I}$. Let $Y\subset V\times\mathcal{I}$ be the set of variables on which $C$ is not constant. Then it is sufficient for the effect of $\mathcal{I}$ on $Y$ to be identifiable in the Pearlean sense for the optimal policy to be found (if it exists). Also sufficient is if the effect of $\mathcal{I}$ on $C(Y)$ to be identifiable.

In both cases it is possible to evaluate the function $h(i):= E_{\Pi_i}[C(Y)]$ at every $i\in\mathcal{I}$, provided this expectation exists. If it is possible to minimize $h$ on $\mathcal{I}$, then an optimal policy can be found.

Using Rubin's weaker definition of identifiability, it is necessary that the effect of $\mathcal{I}$ on $C(Y)$ be identifiable.

\subsubsection{Identifiability and nonconstant policy cost minimisation}

For reasons analogous to those above, if we admit non-constant policies $f:A\to I$ into consideration, it is sufficient to know $P_{f}(y|A=a)$ for all $f\in\mathcal{F}$.

Suppose that the intervention $I$ has no causal effect on $A$. That is, $P_f (A) = P_{f'}(A)$ for any $f,f'\in\mathcal{F}$. This avoids issues of circular dependence, but it is an additional assumption, not a matter of necessity.

If the above assumptions holds, $m$ is a CBN* and the effect of $I$ on $Y$ is identifiable, then the conditional effect of $I$ on $Y$ given $A$ is identifiable.

\textbf{Proof sketch:} First, redo Definition \ref{def:CBN} with conditional interventions.

Next, by parental adjustment, $P_*(y|\mathrm{pa}_Y)$ is constant for all policies $*$ provided $\mathrm{pa}_Y$ has nonzero probability under $\mathrm{pa}_Y$.  

Define $\mathrm{ndp}^Y_I = \mathrm{pa}_Y\cap \mathrm{nd}_I$ and $\mathrm{dep}^Y_I=\mathrm{pa}_Y\cap \mathrm{de}_I$

\begin{align}
    P_*(y|a) &= \sum_{\mathrm{pa}_Y\setminus \{A\}} P_*(y|\mathrm{pa}_Y) P_*(\mathrm{pa}_Y\setminus\{A\}|a) \\
    P_*(y|a) &= \sum_{\mathrm{pa}_Y\setminus \{A\}} P_*(y|\mathrm{pa}_Y) P_*(\mathrm{ndp}^Y_I|a) P_*(\mathrm{dep}^Y_I|\mathrm{ndp}^Y_I,a)
\end{align}

We can split $\mathrm{pa}_Y$ into $\mathrm{pa}_Y\cap \mathrm{de}_I$ and $\mathrm{pa}_Y\cap \mathrm{nd}_I$. $P_*(\mathrm{ndp}_I)$ can only depend on the policy via $a$, and so $P_*(\mathrm{ndp}_I)$ is fixed for all policies. By the assumption that $A\in\mathrm{nd}_I$,  $P_*(\mathrm{dep}^Y_I|\mathrm{ndp}^Y_I,a)$ depends only on the policy via $I$.

Therefore $P_f(y|a)=P_{f'}(y|a)$ for all $f,f'$ such that $f(a)=f'(a)$.

If the effect of $I$ on $Y$ is identifiable, then we know $P_{\Pi_i}(y|a)$ for all constant policies $\Pi_i$. We can therefore compute $P_f(y|a)$ for non-constant $f$ by evaluating the constant policy $P_{\Pi_{f(a)}}(y|a)$ at every value of $a$.

*Needs to be an extended CBN with non-constant interventions

% \subsection{Recovering a model-selection rule}

% Given $f:X\to \mathbb{R}$, define $\text{argmin}_x f(x) = \{x: f(x)\leq f(x')\text{ for all }x'\in X\}$ (this is the standard definition, I just put it here to emphasise that argmin is a set).

% Suppose we have a loss $L_{M^*,C}:\mathcal{M}\to \mathbb{R}$  and a set of models $\mathcal{M}$ with the property that 
% \begin{align}
%     M \in \mathrm{argmin}_M L_{M^*,C}(M)\implies \mathrm{argmin}_I \mathbb{E}_{M(I)}[C(X)] = \mathrm{argmin}_I \mathbb{E}_{M^*(I)}[C(X)] \label{eq:cq_loss1}
% \end{align}
% Then proceed as in Def. \ref{def:causal_query_1} with $D$ replaced by $M^*$.

% Condition \ref{eq:cq_loss1} is too weak - could strengthen it to for every $\epsilon > 0$ there is $\delta > 0$ s.t.
% \begin{align}
%     M \in \{M:L_{M^*,C}(M) < \mathrm{min}_{M'} L_{M^*,C}(M') + \delta \}\implies \mathrm{argmin}_I \mathbb{E}_{M(I)}[C(X)] = \mathrm{argmin}_I \mathbb{E}_{M^*(I)}[C(X)] + \epsilon \label{eq:cq_loss2}
% \end{align}

% Additional complication: given $\mathcal{S}$, we may not be able to find an approximately optimal model $M$. In particular, we may not be able to bound $|L_{M^*,C}(M)-L_{\mathcal{S},C}(M)|$. ($L_{\mathcal{S}}$ needs definition here).

% \textbf{Are there alternative weaker conditions that may be desirable? Do these lead to actually-existing types of causal inference query?}

% It seems like the two things that might be desirable are, informally, policy improvement and sample complexity improvement. 

% \textbf{Weak policy improvement:} given some prior $P_\mathcal{M}$ over $M$ and $P_\mathcal{M'}$ over $M'=\text{argmin}_M L_{\mathcal{S},C}(M)$ and 
% \[I\in \mathrm{argmin}_I \mathbb{E}_{P_{M}}\left[\mathbb{E}_{M^*(I)}[C(X)]\right]\] \[I'\in \mathrm{argmin}_I \mathbb{E}_{P_{M'}}\left[\mathbb{E}_{M(I)}[C(X)]\right]\]
% weak policy improvement:
% \begin{align}
%     \mathbb{E}_{M^*(I')}[C(X)] < \mathbb{E}_{M^*(I)}[C(X)]
% \end{align}
% Note: introducing $P_\mathcal{M}$ and $P_\mathcal{M'}$ suggests replacing the loss $L$ with Bayes rule?

% \textbf{Sample complexity improvement:} suppose we have $L$ such that Eq \ref{eq:cq_loss2} holds, and there exist extra observations $\mathcal{S}_0$ such that for model class $\mathcal{M}$ w.p. $1-\delta$ we have \[|L_{M^*,C}(M)-L_{\mathcal{S}\cup\mathcal{S}_0,C}(M)|<\epsilon\]

% Suppose we have a loss $\tilde{L}_{\mathcal{S},C}$ such that for $\mathcal{M}'=\mathrm{argmin}_M L_{\mathcal{S},C}(M)$ there exist extra observations $\mathcal{S}_0'$ such that $|\mathcal{S}_0'|<|\mathcal{S}_0|$ and
% \[|L_{M^*,C}(M)-L_{\mathcal{S}\cup\mathcal{S}'_0,C}(M)|<\epsilon\]

% ** This needs a universal quantifier.

% \subsection{Inferring a causal model}

% Given data $D$ over variables $\mathbf{V}$, a class of possible causal models $\mathcal{M}$ and a $D$-dependent loss $\mathcal{L}_D:\mathcal{M}\to \mathbb{R}$, which models $M\subset \mathcal{M}$ minimise $\mathcal{L}_D$?

% Examples: Pearl, chapter 2 \cite{pearl_causality:_2009} - uses estimated distribution $\hat{P}$ rather than data $D$. Proposes two algorithms $IC$ and $IC^*$. Both output sets of CBNs with common conditional independence properties.

% $IC$ considers the class $\mathcal{M}\subset\text{CBN}$ which is the union of all causal structures that can be represented as graphs with exactly one node for each $V\in\mathbf{V}$.

% $IC^*$ considers the class that is the union of all causal structures that can be represented as graphs with one node for each $V\in\mathbf{V}$ and one common cause node for each pair of variables.


% \subsection{Predicting the effect of an intervention}

% Given data $D$ over variables $\mathbf{V}$ and intervention space $\mathcal{I}$, outcome variables $\mathbf{Y}\subset\mathbf{V}$ predictor $\hat{M}:\mathcal{I}\to\text{Range}(Y)$ and loss $\mathcal{L}_D:\hat{M}\to\mathbb{R}$, find $\hat{M}$ minimising $\mathcal{L}_D$.*

% What if we want the complete distribution over $Y$ rather than a point estimate?