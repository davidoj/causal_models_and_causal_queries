\section{Causal Policies}

A proposed general framework for decision-theoretic causal queries:

% \begin{definition}[Causal model query]\label{def:causal_query_1}
% Given complete dataset $D$ over domain $V$ and a sample $S\subset D$, a class of causal models $\mathcal{M}$ and a $D$-dependent loss $\mathcal{L}_D:\mathcal{M}\to \mathbb{R}$, using the sample $S$ find a model $M\in \mathcal{M}$ minimising $\mathcal{L}_D$?
% \end{definition}

% Thinking aloud: given a distinction between intervention and other variables $V=X\times \mathcal{I}$, the causal model defined in \ref{def:causal_models} explicitly avoids requiring a marginal distribution $P(\mathcal{I})$, though the conditional $P(X|\mathcal{I}=I)=M(I)$ exists.

% On the other hand, given a complete dataset $D$, $P(\mathcal{I})$ will exist and $\mathcal{L}_D$ will depend on it. It seems undesirable for a causal query to depend on the selection of intervention.

% Try stepping up a level of generality:

% Let $\mathcal{I}$ be an arbitrary set of interventions. If we imagine a robot interacting with the world, we can think of $\mathcal{I}$ as the set of all possible signals the robot could send to its actuators. We will later discuss when it makes sense to consider particular types of intervention such as $do$-type interventions.

Define the collection of conditionals $P(X|\mathcal{I})=\{P(X|I)|I\in\mathcal{I}\}$.

\begin{definition}[Causal policy]\label{def:causal_query_2}
Given a set of conditionals $P(X|\mathcal{I})$ with domain $X$ and interventions $\mathcal{I}$ and a cost function $C:X\times\mathcal{I}\to\mathbb{R}$ find $I$ minimising $\mathbb{E}_{P(X|I)}[C(X,I)]$. The resulting $I$ is the \emph{causal policy}
\end{definition}

\subsection{Problems with causal policies}\label{ssec:prob_causal_policy}

A causal policy (Def \ref{def:causal_query_2}) suggests the existence of an agent with an ability to conduct the interventions $\mathcal{I}$. However, definitions so far leave open the mechanism by which the agent can conduct such interventions. This is undesirable, as if the agent implements a policy using some mechanism $\mu_I$ to conduct intervention $I$, we might justifiably be concerned about whether $P(X|I,\mu_I)=P(X|I)$. This isn't just a theoretical concern - it is easy to come up with examples of situations where $P(X|I,do(I))\neq P(X|I,\neg do(I))$. If we add a node representing $\mu_I$ to the model, we might shift our concern to a potential metamechanism $\mu'_I$ and so forth. It might be intuitively obvious in many cases that we can stop when we reach a given level, but the lack of a formal account is unsatisfying.

\subsection{Functional policy}

My proposed solution is to incorporate a policy function into the world model. This demystifies what exactly "interventions" are - they are policy function outputs.

\begin{definition}[Functional policy]\label{def:fp_cpq}
Suppose we have a domain $X\times\mathcal{I}$, a set of candidate policies $\mathcal{F}:\{X\to \mathcal{I}\}$, a map $m:\mathcal{F}\to \mathcal{P}$ where $\mathcal{P}$ is the space of probability distributions over $X\times\mathcal{I}$ and a cost function $C:X\times\mathcal{I}\to \mathbb{R}$.

The functional policy is
\begin{align}
    \Pi=\mathrm{argmin}_{f\in\mathcal{F}}\mathbb{E}_{m(f)}[C(x,I)]    \label{eq:func_cp}
\end{align}
\end{definition}

Needs work: I think that the domain of functions in $\mathcal{F}$ should probably be restricted to things that happen before the function is computed. I'm reluctant to include time if it can be avoided, so leaving it as a note for now.

Definition \ref{def:fp_cpq} is motivated by the following consideration: Suppose we have a true set of conditionals $P^*(X|\mathcal{I})=\{P^*(X|I):\forall I \in \mathcal{I}\}$. Deducing a policy from \ref{def:causal_query_2} using the model $P(X|\mathcal{I})=P^*(X|\mathcal{I})$ is not sufficient to ensure the chosen policy is actually optimal because we haven't specified a model of how the agent interacts with $\mathcal{I}$. Suppose, instead, we have a map $m^*:\mathcal{F}\to\mathcal{P}$ that is true for all $f\in\mathcal{F}$ and derive an optimal policy $\Pi\in\mathcal{F}$ following \ref{def:fp_cpq} with $m=m^*$. This is sufficient to ensure that $\Pi$ is superior to all other policies in $\mathcal{F}$.

% \begin{remark}
% If the set of distributions $\mathcal{P}$ are able to be represented by structural equation models, all variables are functions of other variables in the model. Intervention variables are then distinguished only by the freedom to choose which function computes them.
% \end{remark}


\subsubsection{Self-optimisation}\label{sssec:self_reflection}

Finding an optimal policy via Definition \ref{def:fp_cpq} is itself the application of a map $Q:\{\mathcal{F}\}\times\{\mathcal{F}\to\mathcal{P}\}\times\{\mathcal{I}\to\mathbb{R}\}\to\{X\to\mathcal{I}\}$ such that $Q(\mathcal{F},m,C)=\Pi$ (with symbols defined as above). 

Introducing a map $G\{X\to X\to \mathcal{I}\} \to \{X\to\mathcal{I}\}$, $G(Q)$ is itself a policy - in fact it is the policy actually implemented by the agent, while the idealised $\Pi:=Q(X)$ will, in general, forget some of the inputs of $G(Q)$. 

It is interesting to ask whether it is possible to set the optimisation problem up so that $\Pi=G(Q)$, and what implications this might have. To do so, we must ensure that $G(Q)\in\mathcal{F}$, and so we must augment the domain $X$ with variables representing $\mathcal{F}$,$m$ and $C$.

Needs work: the fact that the model $m$ must contain a representation of itself is suggestive of an incomputability result. Is there such a result here?

Needs work: Generally, when is it possible to take a policy $\Pi_1:X\to\mathcal{I}$ and produce a simplified policy $\Pi_2:C\to\mathcal{I}$ where $C\subset X$ which achieves the same cost?


\subsection{Functional policies in Bayesian Networks}

The definitions above do not require the probability distributions $\mathcal{P}$ be Markovian. However, Markovian distributions permit easy representation of conditional independence with graphical models, so they make a good basis from which to generate examples.

We will follow the custom of labeling nodes in a graphical model with the output of the function represented by the node, so the policy node will be labeled $I$.

We will also neglect any subtleties raised by section \ref{sssec:self_reflection}.

Needs work: the inclusion of a policy function suggests that $I$ may be locally Markovian with respect to the inputs to the policy function. Need to check the exact characteristics WRT Markovianity.

\subsubsection{Simple case}\label{sssec:simple_case}

Let $\mathcal{I},X=\{0,1\}$ and $C(x,I)=x$.

\begin{center}
\begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
semithick ,
state/.style ={ circle ,top color =white ,
draw , text=blue , minimum width =1 cm}]
\node[state] (A) [left] {$I$};
\node[state] (B) [right = of A] {$X$};
\path (A) edge [] node[above] {} (B);
\end{tikzpicture}
\end{center}

We will partially define the probability distribution with the following structural equation:
\begin{align*}
    X:=I
\end{align*}
Choosing a function that computes $I$ will complete the definition.

Given the dependences indicated, we have as $\mathcal{F}$ the set of constant functions on $\mathcal{I}$. Of these, $I\mapsto0$ minimises $C$.

In this simple case, finding the functional policy reduces to finding the best intervention. 

\subsubsection{Additional dependence}

Let $\mathcal{I},X_1,X_2=\{0,1\}$ and $C(x_1,x_2,I)=x_1$. Suppose $X_2$ is observed before $I$ is computed, so it is a potential input of the policy.

\begin{center}
\begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
semithick ,
state/.style ={ circle ,top color =white ,
draw , text=blue , minimum width =1 cm}]
\node[state] (B) [right] {$X_1$};
\node[state] (A) [above left = 1cm and 3cm of B] {$X_2$};
\node[state] (C) [below left = 1cm and 3cm of B] {$I$};
\path (A) edge [] node[above] {} (B);
\path (C) edge [] node[above] {} (B);
\draw[dashed] (A) -- (C);
\end{tikzpicture}
\end{center}

We will partially define the distribution with:
\begin{align*}
    X_1 := \mathrm{XOR}(I,X_2)
\end{align*}

We could consider policies that are constant on $\{0,1\}$, or maps $\{0,1\}\to\{0,1\}$. Clearly in this case we want $\Pi:I\mapsto X_2$. If we further knew that $P(X_2=0)=1$, then the constant policy $f:I\mapsto 0$ would perform equally well.

\begin{remark}
The optimal policy isn't straightforward to derive using Causal Bayesian Networks. If we consider the setup above where we are permitted to conduct interventions on the node labeled by $I$, under the standard definition of intervention we would assume that it cuts off any dependence of $I$ on $X_2$. This would limit us to what are here described as constant policies, which are not in general optimal. 

We could address this by proposing that $do()$-interventions could depend on the value of $X_2$ even if $I$ does not depend on it directly. This would invalidate some properties of Causal Bayesian Networks.
\end{remark}

\begin{remark}
If the policy depends on $X_2$, we never need to know the full probability distribution to find the optimal policy.
\end{remark}

\subsubsection{Hidden dependence}
Let $\mathcal{I},X_1,X_2=\{0,1\}$ and $C(x_1,x_2,I)=x_1$. Suppose $X_2$ is not observed.

\begin{center}
\begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
semithick ,
state/.style ={ circle ,top color =white ,
draw , text=blue , minimum width =1 cm}]
\node[state] (B) [right] {$X_1$};
\node[state,dashed] (A) [above left = 1cm and 3cm of B] {$X_2$};
\node[state] (C) [below left = 1cm and 3cm of B] {$I$};
\path (C) edge [] node[above] {} (B);
\draw[dashed] (A) -- (B);
\end{tikzpicture}
\end{center}

We will assume the following structural equations
\begin{align*}
    X_2 &:= \mu_{X_2}\\
    X_1 &:= \mathrm{XOR}(I,X_2)
\end{align*}
Where $\mu_{X_2}\sim \mathrm{Bernoulli}(0.2)$.

In this case, the problem appears to be identical to a problem with the dependence structure of \ref{sssec:simple_case}, and the optimal policy is constant $\Pi:I\mapsto 1$.

\subsubsection{Confounded observations}
Let $\mathcal{I},X_2=\{0,1\}$, $X_1=\{0,1,2\}$ and $C(x_1,x_2,I)=x_1$. Suppose $X_2$ is not observed.

Suppose also that we make infinite observations under some fixed policy $f$, and we are then free to impose our preferred policy $\Pi$.

\begin{center}
\begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
semithick ,
state/.style ={ circle ,top color =white ,
draw , text=blue , minimum width =1 cm}]
\node[state] (B) [right] {$X_1$};
\node[state,dashed] (A) [above left = 1cm and 3cm of B] {$X_2$};
\node[state] (C) [below left = 1cm and 3cm of B] {$I$};
\path (C) edge [] node[above] {} (B);
\draw[dashed] (A) -- (B);
\draw (A) -- (C);
\end{tikzpicture}
\end{center}

We will assume the following structural equations
\begin{align*}
    X_2 &:= \mu_{X_2}\\
    X_1 &:= \neg I + X_2 \\
    f   &:= X_2
\end{align*}
Where $\mu_{X_2}\sim \mathrm{Bernoulli}(0.2)$.

The optimal policy will be $\Pi:I\mapsto 0$, but from our observations it will appear that $X_1 \CI I$ which would suggest that $\Pi$ is no better than the alternative.

\subsubsection{Randomisation}
Let $\mathcal{I}\{0,1\}$, $X_1=\{0,1,2\}$, $N=\{0,1,..,i,..,n\}$ and $C(x_1,i,I)=x_1$. Suppose $X_2$ is not observed.

We wish to estimate the distribution of $X_1$ under fixed policies $f_0:I\mapsto 0$ and $f_1:I\mapsto 1$, and we can sample the system $n$ times. In order to do this, we will in fact be sampling under the policy $r:N\mapsto \{0,1\}$.

\begin{center}
\begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
semithick ,
state/.style ={ circle ,top color =white ,
draw , text=blue , minimum width =1 cm}]
\node[state] (B) [right] {$X_1$};
\node[state,dashed] (A) [above left = 1cm and 3cm of B] {$N$};
\node[state] (C) [below left = 1cm and 3cm of B] {$I$};
\path (C) edge [] node[above] {} (B);
\draw[dashed] (A) -- (B);
\draw (A) -- (C);
\end{tikzpicture}
\end{center}

The dependence structure is identical to the previous section, which showed that in general it isn't possible to deduce the optimal policy from these observations. The difference is that we are free to choose $r$.

Suppose there is some set of ``plausible'' functions $\mathcal{H}=\{N\times I\to X_1\}$. If we choose $f'$ such that 
\begin{align}
    \max_{h\in\mathcal{H}} \left| \langle \{h(i,0):r(i)=0\}\rangle - \langle \{h(i,0):i\in\{n\}\} \rangle \right| < \epsilon \label{eq:bin_random}
\end{align}
Where $\langle\cdot\rangle$ denotes an average. If we suppose this also holds for $h(\cdot,1)$, then we can be confident that the subset of samples for which $r(i)=0$ will provide a good estimate for the distribution of $X_1$ under $f_0$, and the subset for which $r(i)=1$ will provide a good estimate for the distribution under $f_1$.




% \subsection{Recovering a model-selection rule}

% Given $f:X\to \mathbb{R}$, define $\text{argmin}_x f(x) = \{x: f(x)\leq f(x')\text{ for all }x'\in X\}$ (this is the standard definition, I just put it here to emphasise that argmin is a set).

% Suppose we have a loss $L_{M^*,C}:\mathcal{M}\to \mathbb{R}$  and a set of models $\mathcal{M}$ with the property that 
% \begin{align}
%     M \in \mathrm{argmin}_M L_{M^*,C}(M)\implies \mathrm{argmin}_I \mathbb{E}_{M(I)}[C(X)] = \mathrm{argmin}_I \mathbb{E}_{M^*(I)}[C(X)] \label{eq:cq_loss1}
% \end{align}
% Then proceed as in Def. \ref{def:causal_query_1} with $D$ replaced by $M^*$.

% Condition \ref{eq:cq_loss1} is too weak - could strengthen it to for every $\epsilon > 0$ there is $\delta > 0$ s.t.
% \begin{align}
%     M \in \{M:L_{M^*,C}(M) < \mathrm{min}_{M'} L_{M^*,C}(M') + \delta \}\implies \mathrm{argmin}_I \mathbb{E}_{M(I)}[C(X)] = \mathrm{argmin}_I \mathbb{E}_{M^*(I)}[C(X)] + \epsilon \label{eq:cq_loss2}
% \end{align}

% Additional complication: given $\mathcal{S}$, we may not be able to find an approximately optimal model $M$. In particular, we may not be able to bound $|L_{M^*,C}(M)-L_{\mathcal{S},C}(M)|$. ($L_{\mathcal{S}}$ needs definition here).

% \textbf{Are there alternative weaker conditions that may be desirable? Do these lead to actually-existing types of causal inference query?}

% It seems like the two things that might be desirable are, informally, policy improvement and sample complexity improvement. 

% \textbf{Weak policy improvement:} given some prior $P_\mathcal{M}$ over $M$ and $P_\mathcal{M'}$ over $M'=\text{argmin}_M L_{\mathcal{S},C}(M)$ and 
% \[I\in \mathrm{argmin}_I \mathbb{E}_{P_{M}}\left[\mathbb{E}_{M^*(I)}[C(X)]\right]\] \[I'\in \mathrm{argmin}_I \mathbb{E}_{P_{M'}}\left[\mathbb{E}_{M(I)}[C(X)]\right]\]
% weak policy improvement:
% \begin{align}
%     \mathbb{E}_{M^*(I')}[C(X)] < \mathbb{E}_{M^*(I)}[C(X)]
% \end{align}
% Note: introducing $P_\mathcal{M}$ and $P_\mathcal{M'}$ suggests replacing the loss $L$ with Bayes rule?

% \textbf{Sample complexity improvement:} suppose we have $L$ such that Eq \ref{eq:cq_loss2} holds, and there exist extra observations $\mathcal{S}_0$ such that for model class $\mathcal{M}$ w.p. $1-\delta$ we have \[|L_{M^*,C}(M)-L_{\mathcal{S}\cup\mathcal{S}_0,C}(M)|<\epsilon\]

% Suppose we have a loss $\tilde{L}_{\mathcal{S},C}$ such that for $\mathcal{M}'=\mathrm{argmin}_M L_{\mathcal{S},C}(M)$ there exist extra observations $\mathcal{S}_0'$ such that $|\mathcal{S}_0'|<|\mathcal{S}_0|$ and
% \[|L_{M^*,C}(M)-L_{\mathcal{S}\cup\mathcal{S}'_0,C}(M)|<\epsilon\]

% ** This needs a universal quantifier.

% \subsection{Inferring a causal model}

% Given data $D$ over variables $\mathbf{V}$, a class of possible causal models $\mathcal{M}$ and a $D$-dependent loss $\mathcal{L}_D:\mathcal{M}\to \mathbb{R}$, which models $M\subset \mathcal{M}$ minimise $\mathcal{L}_D$?

% Examples: Pearl, chapter 2 \cite{pearl_causality:_2009} - uses estimated distribution $\hat{P}$ rather than data $D$. Proposes two algorithms $IC$ and $IC^*$. Both output sets of CBNs with common conditional independence properties.

% $IC$ considers the class $\mathcal{M}\subset\text{CBN}$ which is the union of all causal structures that can be represented as graphs with exactly one node for each $V\in\mathbf{V}$.

% $IC^*$ considers the class that is the union of all causal structures that can be represented as graphs with one node for each $V\in\mathbf{V}$ and one common cause node for each pair of variables.


% \subsection{Predicting the effect of an intervention}

% Given data $D$ over variables $\mathbf{V}$ and intervention space $\mathcal{I}$, outcome variables $\mathbf{Y}\subset\mathbf{V}$ predictor $\hat{M}:\mathcal{I}\to\text{Range}(Y)$ and loss $\mathcal{L}_D:\hat{M}\to\mathbb{R}$, find $\hat{M}$ minimising $\mathcal{L}_D$.*

% What if we want the complete distribution over $Y$ rather than a point estimate?