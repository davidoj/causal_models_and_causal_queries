
@article{lemeire_replacing_2013,
	title = {Replacing {Causal} {Faithfulness} with {Algorithmic} {Independence} of {Conditionals}},
	volume = {23},
	issn = {0924-6495, 1572-8641},
	url = {https://link.springer.com/article/10.1007/s11023-012-9283-1},
	doi = {10.1007/s11023-012-9283-1},
	abstract = {Independence of Conditionals (IC) has recently been proposed as a basic rule for causal structure learning. If a Bayesian network represents the causal structure, its Conditional Probability Distributions (CPDs) should be algorithmically independent. In this paper we compare IC with causal faithfulness (FF), stating that only those conditional independences that are implied by the causal Markov condition hold true. The latter is a basic postulate in common approaches to causal structure learning. The common spirit of FF and IC is to reject causal graphs for which the joint distribution looks ‘non-generic’. The difference lies in the notion of genericity: FF sometimes rejects models just because one of the CPDs is simple, for instance if the CPD describes a deterministic relation. IC does not behave in this undesirable way. It only rejects a model when there is a non-generic relation between different CPDs although each CPD looks generic when considered separately. Moreover, it detects relations between CPDs that cannot be captured by conditional independences. IC therefore helps in distinguishing causal graphs that induce the same conditional independences (i.e., they belong to the same Markov equivalence class). The usual justification for FF implicitly assumes a prior that is a probability density on the parameter space. IC can be justified by Solomonoff’s universal prior, assigning non-zero probability to those points in parameter space that have a finite description. In this way, it favours simple CPDs, and therefore respects Occam’s razor. Since Kolmogorov complexity is uncomputable, IC is not directly applicable in practice. We argue that it is nevertheless helpful, since it has already served as inspiration and justification for novel causal inference algorithms.},
	language = {en},
	number = {2},
	urldate = {2018-05-24},
	journal = {Minds and Machines},
	author = {Lemeire, Jan and Janzing, Dominik},
	month = may,
	year = {2013},
	pages = {227--249},
	file = {Full Text PDF:/home/users/u4533535/Zotero/storage/VSRYINKU/Lemeire and Janzing - 2013 - Replacing Causal Faithfulness with Algorithmic Ind.pdf:application/pdf;Snapshot:/home/users/u4533535/Zotero/storage/DCRU8TNB/s11023-012-9283-1.html:text/html}
}


@book{pearl_causality:_2009,
	edition = {2},
	title = {Causality: {Models}, {Reasoning} and {Inference}},
	publisher = {Cambridge University Press},
	author = {Pearl, Judea},
	year = {2009}
}



@article{chickering_optimal_2003,
	title = {Optimal {Structure} {Identification} with {Greedy} {Search}},
	volume = {3},
	issn = {1532-4435},
	url = {https://doi.org/10.1162/153244303321897717},
	doi = {10.1162/153244303321897717},
	abstract = {In this paper we prove the so-called "Meek Conjecture". In particular, we show that if a DAG H is an independence map of another DAG G, then there exists a finite sequence of edge additions and covered edge reversals in G such that (1) after each edge modification H remains an independence map of G and (2) after all modifications G =H. As shown by Meek (1997), this result has an important consequence for Bayesian approaches to learning Bayesian networks from data: in the limit of large sample size, there exists a two-phase greedy search algorithm that---when applied to a particular sparsely-connected search space---provably identifies a perfect map of the generative distribution if that perfect map is a DAG. We provide a new implementation of the search space, using equivalence classes as states, for which all operators used in the greedy search can be scored efficiently using local functions of the nodes in the domain. Finally, using both synthetic and real-world datasets, we demonstrate that the two-phase greedy approach leads to good solutions when learning with finite sample sizes.},
	urldate = {2018-02-27},
	journal = {J. Mach. Learn. Res.},
	author = {Chickering, David Maxwell},
	month = mar,
	year = {2003},
	pages = {507--554},
	annote = {It's possible to identify the "true" data-generating DAG in the infinite sample limit using a two-phase local search algorithm. This follows from the fact that it's possible to},
	file = {ACM Full Text PDF:/home/users/u4533535/Zotero/storage/FIK5RCSU/Chickering - 2003 - Optimal Structure Identification with Greedy Searc.pdf:application/pdf}
}


@book{spirtes_causation_1993,
	title = {Causation, {Prediction}, and {Search}},
	volume = {81},
	abstract = {What assumptions and methods allow us to turn observations into causal knowledge, and how can even incomplete causal knowledge be used in planning and prediction to influence and control our environment? In this book Peter Spirtes, Clark Glymour, and Richard Scheines address these questions using the formalism of Bayes networks, with results that have been applied in diverse areas of research in the social, behavioral, and physical sciences. The authors show that although experimental and observational study designs may not always permit the same inferences, they are subject to uniform principles. They axiomatize the connection between causal structure and probabilistic independence, explore several varieties of causal indistinguishability, formulate a theory of manipulation, and develop asymptotically reliable procedures for searching over equivalence classes of causal models, including models of categorical data and structural equation models with and without latent variables. The authors show that the relationship between causality and probability can also help to clarify such diverse topics in statistics as the comparative power of experimentation versus observation, Simpson's paradox, errors in regression models, retrospective versus prospective sampling, and variable selection. The second edition contains a new introduction and an extensive survey of advances and applications that have appeared since the first edition was published in 1993.},
	author = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
	month = jan,
	year = {1993},
	doi = {10.1007/978-1-4612-2748-9},
	file = {Full Text PDF:/home/users/u4533535/Zotero/storage/Q5VT529X/Spirtes et al. - 1993 - Causation, Prediction, and Search.pdf:application/pdf}
}


@article{evans_margins_2015,
	title = {Margins of discrete {Bayesian} networks},
	url = {http://arxiv.org/abs/1501.02103},
	abstract = {Bayesian network models with latent variables are widely used in statistics and machine learning. In this paper we provide a complete algebraic characterization of Bayesian network models with latent variables when the observed variables are discrete and no assumption is made about the state-space of the latent variables. We show that it is algebraically equivalent to the so-called nested Markov model, meaning that the two are the same up to inequality constraints on the joint probabilities. In particular these two models have the same dimension. The nested Markov model is therefore the best possible description of the latent variable model that avoids consideration of inequalities, which are extremely complicated in general. A consequence of this is that the constraint finding algorithm of Tian and Pearl (UAI 2002, pp519-527) is complete for finding equality constraints. Latent variable models suffer from difficulties of unidentifiable parameters and non-regular asymptotics; in contrast the nested Markov model is fully identifiable, represents a curved exponential family of known dimension, and can easily be fitted using an explicit parameterization.},
	urldate = {2018-02-20},
	journal = {arXiv:1501.02103 [math, stat]},
	author = {Evans, Robin J.},
	month = jan,
	year = {2015},
	note = {arXiv: 1501.02103},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
	annote = {Comment: 41 pages},
	annote = {Shows that nested markov models have the same dimension as marginal DAGs, which model DAGs with hidden variables. Nested markov models furthermore respect all equality constraints of mDAGs, though mDAGs might respect further inequality constraints; Tian and Pearl (related) show how to derive these equality constraints},
	file = {arXiv\:1501.02103 PDF:/home/users/u4533535/Zotero/storage/L9ALWTEB/Evans - 2015 - Margins of discrete Bayesian networks.pdf:application/pdf;arXiv.org Snapshot:/home/users/u4533535/Zotero/storage/ZAXBMMS3/1501.html:text/html}
}


@article{kang_inequality_2012,
	title = {Inequality {Constraints} in {Causal} {Models} with {Hidden} {Variables}},
	url = {http://arxiv.org/abs/1206.6829},
	abstract = {We present a class of inequality constraints on the set of distributions induced by local interventions on variables governed by a causal Bayesian network, in which some of the variables remain unmeasured. We derive bounds on causal effects that are not directly measured in randomized experiments. We derive instrumental inequality type of constraints on nonexperimental distributions. The results have applications in testing causal models with observational or experimental data.},
	urldate = {2018-02-20},
	journal = {arXiv:1206.6829 [cs, stat]},
	author = {Kang, Changsung and Tian, Jin},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.6829},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Methodology},
	annote = {Comment: Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)},
	file = {arXiv\:1206.6829 PDF:/home/users/u4533535/Zotero/storage/TBWYF3KN/Kang and Tian - 2012 - Inequality Constraints in Causal Models with Hidde.pdf:application/pdf;arXiv.org Snapshot:/home/users/u4533535/Zotero/storage/PNZ4JUK5/1206.html:text/html}
}


@article{yang_characterizing_2018,
	title = {Characterizing and {Learning} {Equivalence} {Classes} of {Causal} {DAGs} under {Interventions}},
	url = {http://arxiv.org/abs/1802.06310},
	abstract = {We consider the problem of learning causal DAGs in the setting where both observational and interventional data is available. This setting is common in biology, where gene regulatory networks can be intervened on using chemical reagents or gene deletions. Hauser and B{\textbackslash}"uhlmann (2012) previously characterized the identifiability of causal DAGs under perfect interventions, which eliminate dependencies between targeted variables and their direct causes. In this paper, we extend these identifiability results to general interventions, which may modify the dependencies between targeted variables and their causes without eliminating them. We define and characterize the interventional Markov equivalence class that can be identified from general (not necessarily perfect) intervention experiments. We also propose the first provably consistent algorithm for learning DAGs in this setting and evaluate our algorithm on simulated and biological datasets.},
	urldate = {2018-06-06},
	journal = {arXiv:1802.06310 [math, stat]},
	author = {Yang, Karren D. and Katcoff, Abigail and Uhler, Caroline},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.06310},
	keywords = {Mathematics - Statistics Theory, Statistics - Applications, Statistics - Methodology},
	annote = {Comment: 18 pages, 7 figures},
	file = {arXiv\:1802.06310 PDF:/home/users/u4533535/Zotero/storage/QYBLHGF6/Yang et al. - 2018 - Characterizing and Learning Equivalence Classes of.pdf:application/pdf;arXiv.org Snapshot:/home/users/u4533535/Zotero/storage/9XLAKWAE/1802.html:text/html}
}


@article{hauser_characterization_2012,
	title = {Characterization and {Greedy} {Learning} of {Interventional} {Markov} {Equivalence} {Classes} of {Directed} {Acyclic} {Graphs}},
	volume = {13},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v13/hauser12a.html},
	number = {Aug},
	urldate = {2018-06-06},
	journal = {Journal of Machine Learning Research},
	author = {Hauser, Alain and Bühlmann, Peter},
	year = {2012},
	pages = {2409--2464},
	file = {Full Text PDF:/home/users/u4533535/Zotero/storage/AR23G9FQ/Hauser and Bühlmann - 2012 - Characterization and Greedy Learning of Interventi.pdf:application/pdf;Snapshot:/home/users/u4533535/Zotero/storage/25U334TM/hauser12a.html:text/html}
}


@article{meek_strong_2013,
	title = {Strong {Completeness} and {Faithfulness} in {Bayesian} {Networks}},
	url = {https://arxiv.org/abs/1302.4973},
	language = {en},
	urldate = {2018-05-21},
	author = {Meek, Christopher},
	month = feb,
	year = {2013},
	file = {Full Text PDF:/home/users/u4533535/Zotero/storage/4JIRGSD9/Meek - 2013 - Strong Completeness and Faithfulness in Bayesian N.pdf:application/pdf}
}


@article{peters_causal_2014,
	title = {Causal {Discovery} with {Continuous} {Additive} {Noise} {Models}},
	volume = {15},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=2627435.2670315},
	abstract = {Loading...},
	number = {1},
	urldate = {2018-05-22},
	journal = {J. Mach. Learn. Res.},
	author = {Peters, Jonas and Mooij, Joris M. and Janzing, Dominik and Schölkopf, Bernhard},
	month = jan,
	year = {2014},
	keywords = {additive noise, Bayesian networks, causal inference, causal minimality, identifiability, structural equation models},
	pages = {2009--2053},
	file = {ACM Full Text PDF:/home/users/u4533535/Zotero/storage/6DNIF3QP/Peters et al. - 2014 - Causal Discovery with Continuous Additive Noise Mo.pdf:application/pdf}
}


@article{yudkowsky_functional_2017,
	title = {Functional {Decision} {Theory}: {A} {New} {Theory} of {Instrumental} {Rationality}},
	shorttitle = {Functional {Decision} {Theory}},
	url = {http://arxiv.org/abs/1710.05060},
	abstract = {This paper describes and motivates a new decision theory known as functional decision theory (FDT), as distinct from causal decision theory and evidential decision theory. Functional decision theorists hold that the normative principle for action is to treat one's decision as the output of a fixed mathematical function that answers the question, "Which output of this very function would yield the best outcome?" Adhering to this principle delivers a number of benefits, including the ability to maximize wealth in an array of traditional decision-theoretic and game-theoretic problems where CDT and EDT perform poorly. Using one simple and coherent decision rule, functional decision theorists (for example) achieve more utility than CDT on Newcomb's problem, more utility than EDT on the smoking lesion problem, and more utility than both in Parfit's hitchhiker problem. In this paper, we define FDT, explore its prescriptions in a number of different decision problems, compare it to CDT and EDT, and give philosophical justifications for FDT as a normative theory of decision-making.},
	urldate = {2018-07-02},
	journal = {arXiv:1710.05060 [cs]},
	author = {Yudkowsky, Eliezer and Soares, Nate},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.05060},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv\:1710.05060 PDF:/home/users/u4533535/Zotero/storage/62TI8YLX/Yudkowsky and Soares - 2017 - Functional Decision Theory A New Theory of Instru.pdf:application/pdf;arXiv.org Snapshot:/home/users/u4533535/Zotero/storage/PQ7QQZMF/1710.html:text/html}
}


@article{galles_testing_2013,
	title = {Testing {Identifiability} of {Causal} {Effects}},
	url = {http://arxiv.org/abs/1302.4948},
	abstract = {This paper concerns the probabilistic evaluation of the effects of actions in the presence of unmeasured variables. We show that the identification of causal effect between a singleton variable X and a set of variables Y can be accomplished systematically, in time polynomial in the number of variables in the graph. When the causal effect is identifiable, a closed-form expression can be obtained for the probability that the action will achieve a specified goal, or a set of goals.},
	urldate = {2018-07-09},
	journal = {arXiv:1302.4948 [cs]},
	author = {Galles, David and Pearl, Judea},
	month = feb,
	year = {2013},
	note = {arXiv: 1302.4948},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)},
	file = {arXiv\:1302.4948 PDF:C\:\\Users\\david\\Zotero\\storage\\R27PJQGJ\\Galles and Pearl - 2013 - Testing Identifiability of Causal Effects.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\david\\Zotero\\storage\\7KEGAEMG\\1302.html:text/html}
}


@article{rubin_causal_2005,
	title = {Causal {Inference} {Using} {Potential} {Outcomes}},
	volume = {100},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214504000001880},
	doi = {10.1198/016214504000001880},
	abstract = {Causal effects are defined as comparisons of potential outcomes under different treatments on a common set of units. Observed values of the potential outcomes are revealed by the assignment mechanism—a probabilistic model for the treatment each unit receives as a function of covariates and potential outcomes. Fisher made tremendous contributions to causal inference through his work on the design of randomized experiments, but the potential outcomes perspective applies to other complex experiments and nonrandomized studies as well. As noted by Kempthorne in his 1976 discussion of Savage's Fisher lecture, Fisher never bridged his work on experimental design and his work on parametric modeling, a bridge that appears nearly automatic with an appropriate view of the potential outcomes framework, where the potential outcomes and covariates are given a Bayesian distribution to complete the model specification. Also, this framework crisply separates scientific inference for causal effects and decisions based on such inference, a distinction evident in Fisher's discussion of tests of significance versus tests in an accept/reject framework. But Fisher never used the potential outcomes framework, originally proposed by Neyman in the context of randomized experiments, and as a result he provided generally flawed advice concerning the use of the analysis of covariance to adjust for posttreatment concomitants in randomized trials.},
	number = {469},
	urldate = {2018-07-09},
	journal = {Journal of the American Statistical Association},
	author = {Rubin, Donald B.},
	month = mar,
	year = {2005},
	keywords = {Analysis of covariance, Assignment mechanism, Assignment-based causal inference, Bayesian inference, Direct causal effects, Fieller–Creasy, Fisher, Neyman, Observational studies, Principal stratification, Randomized experiments, Rubin causal model},
	pages = {322--331},
	file = {Full Text PDF:C\:\\Users\\david\\Zotero\\storage\\UCYCX9EZ\\Rubin - 2005 - Causal Inference Using Potential Outcomes.pdf:application/pdf;Snapshot:C\:\\Users\\david\\Zotero\\storage\\FL3D7Q29\\016214504000001880.html:text/html}
}


@article{schurz_causality_2016,
	title = {Causality as a theoretical concept: explanatory warrant and empirical content of the theory of causal nets},
	volume = {193},
	issn = {0039-7857, 1573-0964},
	shorttitle = {Causality as a theoretical concept},
	url = {https://link.springer.com/article/10.1007/s11229-014-0630-z},
	doi = {10.1007/s11229-014-0630-z},
	abstract = {We start this paper by arguing that causality should, in analogy with force in Newtonian physics, be understood as a theoretical concept that is not explicated by a single definition, but by the axioms of a theory. Such an understanding of causality implicitly underlies the well-known theory of causal (Bayes) nets (TCN) and has been explicitly promoted by Glymour (Br J Philos Sci 55:779–790, 2004). In this paper we investigate the explanatory warrant and empirical content of TCN. We sketch how the assumption of directed cause–effect relations can be philosophically justified by an inference to the best explanation. We then ask whether the explanations provided by TCN are merely post-facto or have independently testable empirical content. To answer this question we develop a fine-grained axiomatization of TCN, including a distinction of different kinds of faithfulness. A number of theorems show that although the core axioms of TCN are empirically empty, extended versions of TCN have successively increasing empirical content.},
	language = {en},
	number = {4},
	urldate = {2018-03-27},
	journal = {Synthese},
	author = {Schurz, Gerhard and Gebharter, Alexander},
	month = apr,
	year = {2016},
	pages = {1073--1103},
	file = {Full Text PDF:C\:\\Users\\david\\Zotero\\storage\\Y6W8FAUJ\\Schurz and Gebharter - 2016 - Causality as a theoretical concept explanatory wa.pdf:application/pdf;Snapshot:C\:\\Users\\david\\Zotero\\storage\\ZXGFDGPV\\s11229-014-0630-z.html:text/html}
}


@article{ramsey_adjacency-faithfulness_2012,
	title = {Adjacency-{Faithfulness} and {Conservative} {Causal} {Inference}},
	url = {http://arxiv.org/abs/1206.6843},
	abstract = {Most causal inference algorithms in the literature (e.g., Pearl (2000), Spirtes et al. (2000), Heckerman et al. (1999)) exploit an assumption usually referred to as the causal Faithfulness or Stability condition. In this paper, we highlight two components of the condition used in constraint-based algorithms, which we call "Adjacency-Faithfulness" and "Orientation-Faithfulness". We point out that assuming Adjacency-Faithfulness is true, it is in principle possible to test the validity of Orientation-Faithfulness. Based on this observation, we explore the consequence of making only the Adjacency-Faithfulness assumption. We show that the familiar PC algorithm has to be modified to be (asymptotically) correct under the weaker, Adjacency-Faithfulness assumption. Roughly the modified algorithm, called Conservative PC (CPC), checks whether Orientation-Faithfulness holds in the orientation phase, and if not, avoids drawing certain causal conclusions the PC algorithm would draw. However, if the stronger, standard causal Faithfulness condition actually obtains, the CPC algorithm is shown to output the same pattern as the PC algorithm does in the large sample limit. We also present a simulation study showing that the CPC algorithm runs almost as fast as the PC algorithm, and outputs significantly fewer false causal arrowheads than the PC algorithm does on realistic sample sizes. We end our paper by discussing how score-based algorithms such as GES perform when the Adjacency-Faithfulness but not the standard causal Faithfulness condition holds, and how to extend our work to the FCI algorithm, which allows for the possibility of latent variables.},
	urldate = {2018-02-26},
	journal = {arXiv:1206.6843 [cs, stat]},
	author = {Ramsey, Joseph and Zhang, Jiji and Spirtes, Peter L.},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.6843},
	keywords = {Statistics - Methodology, Computer Science - Artificial Intelligence},
	file = {arXiv\:1206.6843 PDF:C\:\\Users\\david\\Zotero\\storage\\5J3TGVYR\\Ramsey et al. - 2012 - Adjacency-Faithfulness and Conservative Causal Inf.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\david\\Zotero\\storage\\8VWWK4MQ\\1206.html:text/html}
}


@misc{sontag_causal_nodate,
	title = {Causal {Inference} for {Observational} {Studies}: {ICML} 2016 {Tutorial}},
	url = {https://cs.nyu.edu/~shalit/tutorial.html},
	urldate = {2018-01-26},
	author = {Sontag, David and Shalit, Uri},
	file = {Causal Inference for Observational Studies\: ICML 2016 Tutorial:/home/david/Zotero/storage/QMLPNVMY/tutorial.html:text/html}
}


@article{uhler_geometry_2013,
	title = {Geometry of the faithfulness assumption in causal inference},
	volume = {41},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1207.0547},
	doi = {10.1214/12-AOS1080},
	abstract = {Many algorithms for inferring causality rely heavily on the faithfulness assumption. The main justification for imposing this assumption is that the set of unfaithful distributions has Lebesgue measure zero, since it can be seen as a collection of hypersurfaces in a hypercube. However, due to sampling error the faithfulness condition alone is not sufficient for statistical estimation, and strong-faithfulness has been proposed and assumed to achieve uniform or high-dimensional consistency. In contrast to the plain faithfulness assumption, the set of distributions that is not strong-faithful has nonzero Lebesgue measure and in fact, can be surprisingly large as we show in this paper. We study the strong-faithfulness condition from a geometric and combinatorial point of view and give upper and lower bounds on the Lebesgue measure of strong-faithful distributions for various classes of directed acyclic graphs. Our results imply fundamental limitations for the PC-algorithm and potentially also for other algorithms based on partial correlation testing in the Gaussian case.},
	number = {2},
	urldate = {2018-03-26},
	journal = {The Annals of Statistics},
	author = {Uhler, Caroline and Raskutti, Garvesh and Bühlmann, Peter and Yu, Bin},
	month = apr,
	year = {2013},
	note = {arXiv: 1207.0547},
	keywords = {Mathematics - Statistics Theory},
	pages = {436--463},
	annote = {For linear models with Gaussian noise, strong-faithfulness fails for {\textasciitilde}all models with {\textgreater}10 variables.
 
Comment: Published in at http://dx.doi.org/10.1214/12-AOS1080 the Annals of Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical Statistics (http://www.imstat.org)},
	file = {arXiv\:1207.0547 PDF:/home/users/u4533535/Zotero/storage/JSVPBL65/Uhler et al. - 2013 - Geometry of the faithfulness assumption in causal .pdf:application/pdf;arXiv.org Snapshot:/home/users/u4533535/Zotero/storage/CA726KEA/1207.html:text/html}
}


@inproceedings{yang_characterizing_2018,
	title = {Characterizing and {Learning} {Equivalence} {Classes} of {Causal} {DAGs} under {Interventions}},
	url = {http://proceedings.mlr.press/v80/yang18a.html},
	abstract = {We consider the problem of learning causal DAGs in the setting where both observational and interventional data is available. This setting is common in biology, where gene regulatory networks can b...},
	language = {en},
	urldate = {2018-07-19},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Yang, Karren and Katoff, Abigail and Uhler, Caroline},
	month = jul,
	year = {2018},
	pages = {5537--5546},
	file = {Full Text PDF:/home/users/u4533535/Zotero/storage/S8EP4UAQ/Yang et al. - 2018 - Characterizing and Learning Equivalence Classes of.pdf:application/pdf;Snapshot:/home/users/u4533535/Zotero/storage/FLR3XHXJ/yang18a.html:text/html}
}


@article{collaboration_cochrane_nodate,
	title = {Cochrane {Reviewers}' {Handbook} 4.2.1},
	author = {Collaboration, The Cochrane},
	pages = {241},
	file = {Collaboration - Cochrane Reviewers' Handbook 4.2.1.pdf:/home/users/u4533535/Zotero/storage/P4V3SIQJ/Collaboration - Cochrane Reviewers' Handbook 4.2.1.pdf:application/pdf}
}