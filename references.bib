
@article{lemeire_replacing_2013,
	title = {Replacing {Causal} {Faithfulness} with {Algorithmic} {Independence} of {Conditionals}},
	volume = {23},
	issn = {0924-6495, 1572-8641},
	url = {https://link.springer.com/article/10.1007/s11023-012-9283-1},
	doi = {10.1007/s11023-012-9283-1},
	abstract = {Independence of Conditionals (IC) has recently been proposed as a basic rule for causal structure learning. If a Bayesian network represents the causal structure, its Conditional Probability Distributions (CPDs) should be algorithmically independent. In this paper we compare IC with causal faithfulness (FF), stating that only those conditional independences that are implied by the causal Markov condition hold true. The latter is a basic postulate in common approaches to causal structure learning. The common spirit of FF and IC is to reject causal graphs for which the joint distribution looks ‘non-generic’. The difference lies in the notion of genericity: FF sometimes rejects models just because one of the CPDs is simple, for instance if the CPD describes a deterministic relation. IC does not behave in this undesirable way. It only rejects a model when there is a non-generic relation between different CPDs although each CPD looks generic when considered separately. Moreover, it detects relations between CPDs that cannot be captured by conditional independences. IC therefore helps in distinguishing causal graphs that induce the same conditional independences (i.e., they belong to the same Markov equivalence class). The usual justification for FF implicitly assumes a prior that is a probability density on the parameter space. IC can be justified by Solomonoff’s universal prior, assigning non-zero probability to those points in parameter space that have a finite description. In this way, it favours simple CPDs, and therefore respects Occam’s razor. Since Kolmogorov complexity is uncomputable, IC is not directly applicable in practice. We argue that it is nevertheless helpful, since it has already served as inspiration and justification for novel causal inference algorithms.},
	language = {en},
	number = {2},
	urldate = {2018-05-24},
	journal = {Minds and Machines},
	author = {Lemeire, Jan and Janzing, Dominik},
	month = may,
	year = {2013},
	pages = {227--249},
	file = {Full Text PDF:/home/users/u4533535/Zotero/storage/VSRYINKU/Lemeire and Janzing - 2013 - Replacing Causal Faithfulness with Algorithmic Ind.pdf:application/pdf;Snapshot:/home/users/u4533535/Zotero/storage/DCRU8TNB/s11023-012-9283-1.html:text/html}
}


@book{pearl_causality:_2009,
	edition = {2},
	title = {Causality: {Models}, {Reasoning} and {Inference}},
	publisher = {Cambridge University Press},
	author = {Pearl, Judea},
	year = {2009}
}



@article{chickering_optimal_2003,
	title = {Optimal {Structure} {Identification} with {Greedy} {Search}},
	volume = {3},
	issn = {1532-4435},
	url = {https://doi.org/10.1162/153244303321897717},
	doi = {10.1162/153244303321897717},
	abstract = {In this paper we prove the so-called "Meek Conjecture". In particular, we show that if a DAG H is an independence map of another DAG G, then there exists a finite sequence of edge additions and covered edge reversals in G such that (1) after each edge modification H remains an independence map of G and (2) after all modifications G =H. As shown by Meek (1997), this result has an important consequence for Bayesian approaches to learning Bayesian networks from data: in the limit of large sample size, there exists a two-phase greedy search algorithm that---when applied to a particular sparsely-connected search space---provably identifies a perfect map of the generative distribution if that perfect map is a DAG. We provide a new implementation of the search space, using equivalence classes as states, for which all operators used in the greedy search can be scored efficiently using local functions of the nodes in the domain. Finally, using both synthetic and real-world datasets, we demonstrate that the two-phase greedy approach leads to good solutions when learning with finite sample sizes.},
	urldate = {2018-02-27},
	journal = {J. Mach. Learn. Res.},
	author = {Chickering, David Maxwell},
	month = mar,
	year = {2003},
	pages = {507--554},
	annote = {It's possible to identify the "true" data-generating DAG in the infinite sample limit using a two-phase local search algorithm. This follows from the fact that it's possible to},
	file = {ACM Full Text PDF:/home/users/u4533535/Zotero/storage/FIK5RCSU/Chickering - 2003 - Optimal Structure Identification with Greedy Searc.pdf:application/pdf}
}


@book{spirtes_causation_1993,
	title = {Causation, {Prediction}, and {Search}},
	volume = {81},
	abstract = {What assumptions and methods allow us to turn observations into causal knowledge, and how can even incomplete causal knowledge be used in planning and prediction to influence and control our environment? In this book Peter Spirtes, Clark Glymour, and Richard Scheines address these questions using the formalism of Bayes networks, with results that have been applied in diverse areas of research in the social, behavioral, and physical sciences. The authors show that although experimental and observational study designs may not always permit the same inferences, they are subject to uniform principles. They axiomatize the connection between causal structure and probabilistic independence, explore several varieties of causal indistinguishability, formulate a theory of manipulation, and develop asymptotically reliable procedures for searching over equivalence classes of causal models, including models of categorical data and structural equation models with and without latent variables. The authors show that the relationship between causality and probability can also help to clarify such diverse topics in statistics as the comparative power of experimentation versus observation, Simpson's paradox, errors in regression models, retrospective versus prospective sampling, and variable selection. The second edition contains a new introduction and an extensive survey of advances and applications that have appeared since the first edition was published in 1993.},
	author = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
	month = jan,
	year = {1993},
	doi = {10.1007/978-1-4612-2748-9},
	file = {Full Text PDF:/home/users/u4533535/Zotero/storage/Q5VT529X/Spirtes et al. - 1993 - Causation, Prediction, and Search.pdf:application/pdf}
}


@article{evans_margins_2015,
	title = {Margins of discrete {Bayesian} networks},
	url = {http://arxiv.org/abs/1501.02103},
	abstract = {Bayesian network models with latent variables are widely used in statistics and machine learning. In this paper we provide a complete algebraic characterization of Bayesian network models with latent variables when the observed variables are discrete and no assumption is made about the state-space of the latent variables. We show that it is algebraically equivalent to the so-called nested Markov model, meaning that the two are the same up to inequality constraints on the joint probabilities. In particular these two models have the same dimension. The nested Markov model is therefore the best possible description of the latent variable model that avoids consideration of inequalities, which are extremely complicated in general. A consequence of this is that the constraint finding algorithm of Tian and Pearl (UAI 2002, pp519-527) is complete for finding equality constraints. Latent variable models suffer from difficulties of unidentifiable parameters and non-regular asymptotics; in contrast the nested Markov model is fully identifiable, represents a curved exponential family of known dimension, and can easily be fitted using an explicit parameterization.},
	urldate = {2018-02-20},
	journal = {arXiv:1501.02103 [math, stat]},
	author = {Evans, Robin J.},
	month = jan,
	year = {2015},
	note = {arXiv: 1501.02103},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
	annote = {Comment: 41 pages},
	annote = {Shows that nested markov models have the same dimension as marginal DAGs, which model DAGs with hidden variables. Nested markov models furthermore respect all equality constraints of mDAGs, though mDAGs might respect further inequality constraints; Tian and Pearl (related) show how to derive these equality constraints},
	file = {arXiv\:1501.02103 PDF:/home/users/u4533535/Zotero/storage/L9ALWTEB/Evans - 2015 - Margins of discrete Bayesian networks.pdf:application/pdf;arXiv.org Snapshot:/home/users/u4533535/Zotero/storage/ZAXBMMS3/1501.html:text/html}
}


@article{kang_inequality_2012,
	title = {Inequality {Constraints} in {Causal} {Models} with {Hidden} {Variables}},
	url = {http://arxiv.org/abs/1206.6829},
	abstract = {We present a class of inequality constraints on the set of distributions induced by local interventions on variables governed by a causal Bayesian network, in which some of the variables remain unmeasured. We derive bounds on causal effects that are not directly measured in randomized experiments. We derive instrumental inequality type of constraints on nonexperimental distributions. The results have applications in testing causal models with observational or experimental data.},
	urldate = {2018-02-20},
	journal = {arXiv:1206.6829 [cs, stat]},
	author = {Kang, Changsung and Tian, Jin},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.6829},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Methodology},
	annote = {Comment: Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)},
	file = {arXiv\:1206.6829 PDF:/home/users/u4533535/Zotero/storage/TBWYF3KN/Kang and Tian - 2012 - Inequality Constraints in Causal Models with Hidde.pdf:application/pdf;arXiv.org Snapshot:/home/users/u4533535/Zotero/storage/PNZ4JUK5/1206.html:text/html}
}


@article{yang_characterizing_2018,
	title = {Characterizing and {Learning} {Equivalence} {Classes} of {Causal} {DAGs} under {Interventions}},
	url = {http://arxiv.org/abs/1802.06310},
	abstract = {We consider the problem of learning causal DAGs in the setting where both observational and interventional data is available. This setting is common in biology, where gene regulatory networks can be intervened on using chemical reagents or gene deletions. Hauser and B{\textbackslash}"uhlmann (2012) previously characterized the identifiability of causal DAGs under perfect interventions, which eliminate dependencies between targeted variables and their direct causes. In this paper, we extend these identifiability results to general interventions, which may modify the dependencies between targeted variables and their causes without eliminating them. We define and characterize the interventional Markov equivalence class that can be identified from general (not necessarily perfect) intervention experiments. We also propose the first provably consistent algorithm for learning DAGs in this setting and evaluate our algorithm on simulated and biological datasets.},
	urldate = {2018-06-06},
	journal = {arXiv:1802.06310 [math, stat]},
	author = {Yang, Karren D. and Katcoff, Abigail and Uhler, Caroline},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.06310},
	keywords = {Mathematics - Statistics Theory, Statistics - Applications, Statistics - Methodology},
	annote = {Comment: 18 pages, 7 figures},
	file = {arXiv\:1802.06310 PDF:/home/users/u4533535/Zotero/storage/QYBLHGF6/Yang et al. - 2018 - Characterizing and Learning Equivalence Classes of.pdf:application/pdf;arXiv.org Snapshot:/home/users/u4533535/Zotero/storage/9XLAKWAE/1802.html:text/html}
}


@article{hauser_characterization_2012,
	title = {Characterization and {Greedy} {Learning} of {Interventional} {Markov} {Equivalence} {Classes} of {Directed} {Acyclic} {Graphs}},
	volume = {13},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v13/hauser12a.html},
	number = {Aug},
	urldate = {2018-06-06},
	journal = {Journal of Machine Learning Research},
	author = {Hauser, Alain and Bühlmann, Peter},
	year = {2012},
	pages = {2409--2464},
	file = {Full Text PDF:/home/users/u4533535/Zotero/storage/AR23G9FQ/Hauser and Bühlmann - 2012 - Characterization and Greedy Learning of Interventi.pdf:application/pdf;Snapshot:/home/users/u4533535/Zotero/storage/25U334TM/hauser12a.html:text/html}
}


@article{meek_strong_2013,
	title = {Strong {Completeness} and {Faithfulness} in {Bayesian} {Networks}},
	url = {https://arxiv.org/abs/1302.4973},
	language = {en},
	urldate = {2018-05-21},
	author = {Meek, Christopher},
	month = feb,
	year = {2013},
	file = {Full Text PDF:/home/users/u4533535/Zotero/storage/4JIRGSD9/Meek - 2013 - Strong Completeness and Faithfulness in Bayesian N.pdf:application/pdf}
}


@article{peters_causal_2014,
	title = {Causal {Discovery} with {Continuous} {Additive} {Noise} {Models}},
	volume = {15},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=2627435.2670315},
	abstract = {Loading...},
	number = {1},
	urldate = {2018-05-22},
	journal = {J. Mach. Learn. Res.},
	author = {Peters, Jonas and Mooij, Joris M. and Janzing, Dominik and Schölkopf, Bernhard},
	month = jan,
	year = {2014},
	keywords = {additive noise, Bayesian networks, causal inference, causal minimality, identifiability, structural equation models},
	pages = {2009--2053},
	file = {ACM Full Text PDF:/home/users/u4533535/Zotero/storage/6DNIF3QP/Peters et al. - 2014 - Causal Discovery with Continuous Additive Noise Mo.pdf:application/pdf}
}


@article{yudkowsky_functional_2017,
	title = {Functional {Decision} {Theory}: {A} {New} {Theory} of {Instrumental} {Rationality}},
	shorttitle = {Functional {Decision} {Theory}},
	url = {http://arxiv.org/abs/1710.05060},
	abstract = {This paper describes and motivates a new decision theory known as functional decision theory (FDT), as distinct from causal decision theory and evidential decision theory. Functional decision theorists hold that the normative principle for action is to treat one's decision as the output of a fixed mathematical function that answers the question, "Which output of this very function would yield the best outcome?" Adhering to this principle delivers a number of benefits, including the ability to maximize wealth in an array of traditional decision-theoretic and game-theoretic problems where CDT and EDT perform poorly. Using one simple and coherent decision rule, functional decision theorists (for example) achieve more utility than CDT on Newcomb's problem, more utility than EDT on the smoking lesion problem, and more utility than both in Parfit's hitchhiker problem. In this paper, we define FDT, explore its prescriptions in a number of different decision problems, compare it to CDT and EDT, and give philosophical justifications for FDT as a normative theory of decision-making.},
	urldate = {2018-07-02},
	journal = {arXiv:1710.05060 [cs]},
	author = {Yudkowsky, Eliezer and Soares, Nate},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.05060},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv\:1710.05060 PDF:/home/users/u4533535/Zotero/storage/62TI8YLX/Yudkowsky and Soares - 2017 - Functional Decision Theory A New Theory of Instru.pdf:application/pdf;arXiv.org Snapshot:/home/users/u4533535/Zotero/storage/PQ7QQZMF/1710.html:text/html}
}


@article{galles_testing_2013,
	title = {Testing {Identifiability} of {Causal} {Effects}},
	url = {http://arxiv.org/abs/1302.4948},
	abstract = {This paper concerns the probabilistic evaluation of the effects of actions in the presence of unmeasured variables. We show that the identification of causal effect between a singleton variable X and a set of variables Y can be accomplished systematically, in time polynomial in the number of variables in the graph. When the causal effect is identifiable, a closed-form expression can be obtained for the probability that the action will achieve a specified goal, or a set of goals.},
	urldate = {2018-07-09},
	journal = {arXiv:1302.4948 [cs]},
	author = {Galles, David and Pearl, Judea},
	month = feb,
	year = {2013},
	note = {arXiv: 1302.4948},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)},
	file = {arXiv\:1302.4948 PDF:C\:\\Users\\david\\Zotero\\storage\\R27PJQGJ\\Galles and Pearl - 2013 - Testing Identifiability of Causal Effects.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\david\\Zotero\\storage\\7KEGAEMG\\1302.html:text/html}
}


@article{rubin_causal_2005,
	title = {Causal {Inference} {Using} {Potential} {Outcomes}},
	volume = {100},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214504000001880},
	doi = {10.1198/016214504000001880},
	abstract = {Causal effects are defined as comparisons of potential outcomes under different treatments on a common set of units. Observed values of the potential outcomes are revealed by the assignment mechanism—a probabilistic model for the treatment each unit receives as a function of covariates and potential outcomes. Fisher made tremendous contributions to causal inference through his work on the design of randomized experiments, but the potential outcomes perspective applies to other complex experiments and nonrandomized studies as well. As noted by Kempthorne in his 1976 discussion of Savage's Fisher lecture, Fisher never bridged his work on experimental design and his work on parametric modeling, a bridge that appears nearly automatic with an appropriate view of the potential outcomes framework, where the potential outcomes and covariates are given a Bayesian distribution to complete the model specification. Also, this framework crisply separates scientific inference for causal effects and decisions based on such inference, a distinction evident in Fisher's discussion of tests of significance versus tests in an accept/reject framework. But Fisher never used the potential outcomes framework, originally proposed by Neyman in the context of randomized experiments, and as a result he provided generally flawed advice concerning the use of the analysis of covariance to adjust for posttreatment concomitants in randomized trials.},
	number = {469},
	urldate = {2018-07-09},
	journal = {Journal of the American Statistical Association},
	author = {Rubin, Donald B.},
	month = mar,
	year = {2005},
	keywords = {Analysis of covariance, Assignment mechanism, Assignment-based causal inference, Bayesian inference, Direct causal effects, Fieller–Creasy, Fisher, Neyman, Observational studies, Principal stratification, Randomized experiments, Rubin causal model},
	pages = {322--331},
	file = {Full Text PDF:C\:\\Users\\david\\Zotero\\storage\\UCYCX9EZ\\Rubin - 2005 - Causal Inference Using Potential Outcomes.pdf:application/pdf;Snapshot:C\:\\Users\\david\\Zotero\\storage\\FL3D7Q29\\016214504000001880.html:text/html}
}


@article{schurz_causality_2016,
	title = {Causality as a theoretical concept: explanatory warrant and empirical content of the theory of causal nets},
	volume = {193},
	issn = {0039-7857, 1573-0964},
	shorttitle = {Causality as a theoretical concept},
	url = {https://link.springer.com/article/10.1007/s11229-014-0630-z},
	doi = {10.1007/s11229-014-0630-z},
	abstract = {We start this paper by arguing that causality should, in analogy with force in Newtonian physics, be understood as a theoretical concept that is not explicated by a single definition, but by the axioms of a theory. Such an understanding of causality implicitly underlies the well-known theory of causal (Bayes) nets (TCN) and has been explicitly promoted by Glymour (Br J Philos Sci 55:779–790, 2004). In this paper we investigate the explanatory warrant and empirical content of TCN. We sketch how the assumption of directed cause–effect relations can be philosophically justified by an inference to the best explanation. We then ask whether the explanations provided by TCN are merely post-facto or have independently testable empirical content. To answer this question we develop a fine-grained axiomatization of TCN, including a distinction of different kinds of faithfulness. A number of theorems show that although the core axioms of TCN are empirically empty, extended versions of TCN have successively increasing empirical content.},
	language = {en},
	number = {4},
	urldate = {2018-03-27},
	journal = {Synthese},
	author = {Schurz, Gerhard and Gebharter, Alexander},
	month = apr,
	year = {2016},
	pages = {1073--1103},
	file = {Full Text PDF:C\:\\Users\\david\\Zotero\\storage\\Y6W8FAUJ\\Schurz and Gebharter - 2016 - Causality as a theoretical concept explanatory wa.pdf:application/pdf;Snapshot:C\:\\Users\\david\\Zotero\\storage\\ZXGFDGPV\\s11229-014-0630-z.html:text/html}
}


@article{ramsey_adjacency-faithfulness_2012,
	title = {Adjacency-{Faithfulness} and {Conservative} {Causal} {Inference}},
	url = {http://arxiv.org/abs/1206.6843},
	abstract = {Most causal inference algorithms in the literature (e.g., Pearl (2000), Spirtes et al. (2000), Heckerman et al. (1999)) exploit an assumption usually referred to as the causal Faithfulness or Stability condition. In this paper, we highlight two components of the condition used in constraint-based algorithms, which we call "Adjacency-Faithfulness" and "Orientation-Faithfulness". We point out that assuming Adjacency-Faithfulness is true, it is in principle possible to test the validity of Orientation-Faithfulness. Based on this observation, we explore the consequence of making only the Adjacency-Faithfulness assumption. We show that the familiar PC algorithm has to be modified to be (asymptotically) correct under the weaker, Adjacency-Faithfulness assumption. Roughly the modified algorithm, called Conservative PC (CPC), checks whether Orientation-Faithfulness holds in the orientation phase, and if not, avoids drawing certain causal conclusions the PC algorithm would draw. However, if the stronger, standard causal Faithfulness condition actually obtains, the CPC algorithm is shown to output the same pattern as the PC algorithm does in the large sample limit. We also present a simulation study showing that the CPC algorithm runs almost as fast as the PC algorithm, and outputs significantly fewer false causal arrowheads than the PC algorithm does on realistic sample sizes. We end our paper by discussing how score-based algorithms such as GES perform when the Adjacency-Faithfulness but not the standard causal Faithfulness condition holds, and how to extend our work to the FCI algorithm, which allows for the possibility of latent variables.},
	urldate = {2018-02-26},
	journal = {arXiv:1206.6843 [cs, stat]},
	author = {Ramsey, Joseph and Zhang, Jiji and Spirtes, Peter L.},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.6843},
	keywords = {Statistics - Methodology, Computer Science - Artificial Intelligence},
	file = {arXiv\:1206.6843 PDF:C\:\\Users\\david\\Zotero\\storage\\5J3TGVYR\\Ramsey et al. - 2012 - Adjacency-Faithfulness and Conservative Causal Inf.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\david\\Zotero\\storage\\8VWWK4MQ\\1206.html:text/html}
}


@misc{sontag_causal_nodate,
	title = {Causal {Inference} for {Observational} {Studies}: {ICML} 2016 {Tutorial}},
	url = {https://cs.nyu.edu/~shalit/tutorial.html},
	urldate = {2018-01-26},
	author = {Sontag, David and Shalit, Uri},
	file = {Causal Inference for Observational Studies\: ICML 2016 Tutorial:/home/david/Zotero/storage/QMLPNVMY/tutorial.html:text/html}
}


@article{uhler_geometry_2013,
	title = {Geometry of the faithfulness assumption in causal inference},
	volume = {41},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1207.0547},
	doi = {10.1214/12-AOS1080},
	abstract = {Many algorithms for inferring causality rely heavily on the faithfulness assumption. The main justification for imposing this assumption is that the set of unfaithful distributions has Lebesgue measure zero, since it can be seen as a collection of hypersurfaces in a hypercube. However, due to sampling error the faithfulness condition alone is not sufficient for statistical estimation, and strong-faithfulness has been proposed and assumed to achieve uniform or high-dimensional consistency. In contrast to the plain faithfulness assumption, the set of distributions that is not strong-faithful has nonzero Lebesgue measure and in fact, can be surprisingly large as we show in this paper. We study the strong-faithfulness condition from a geometric and combinatorial point of view and give upper and lower bounds on the Lebesgue measure of strong-faithful distributions for various classes of directed acyclic graphs. Our results imply fundamental limitations for the PC-algorithm and potentially also for other algorithms based on partial correlation testing in the Gaussian case.},
	number = {2},
	urldate = {2018-03-26},
	journal = {The Annals of Statistics},
	author = {Uhler, Caroline and Raskutti, Garvesh and Bühlmann, Peter and Yu, Bin},
	month = apr,
	year = {2013},
	note = {arXiv: 1207.0547},
	keywords = {Mathematics - Statistics Theory},
	pages = {436--463},
	annote = {For linear models with Gaussian noise, strong-faithfulness fails for {\textasciitilde}all models with {\textgreater}10 variables.
 
Comment: Published in at http://dx.doi.org/10.1214/12-AOS1080 the Annals of Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical Statistics (http://www.imstat.org)},
	file = {arXiv\:1207.0547 PDF:/home/users/u4533535/Zotero/storage/JSVPBL65/Uhler et al. - 2013 - Geometry of the faithfulness assumption in causal .pdf:application/pdf;arXiv.org Snapshot:/home/users/u4533535/Zotero/storage/CA726KEA/1207.html:text/html}
}




@article{collaboration_cochrane_nodate,
	title = {Cochrane {Reviewers}' {Handbook} 4.2.1},
	author = {Collaboration, The Cochrane},
	pages = {241},
	file = {Collaboration - Cochrane Reviewers' Handbook 4.2.1.pdf:/home/users/u4533535/Zotero/storage/P4V3SIQJ/Collaboration - Cochrane Reviewers' Handbook 4.2.1.pdf:application/pdf}
}


@book{cinlar_probability_2011,
	address = {New York},
	series = {Graduate {Texts} in {Mathematics}},
	title = {Probability and {Stochastics}},
	isbn = {978-0-387-87858-4},
	url = {//www.springer.com/gp/book/9780387878584},
	abstract = {This text is an introduction to the modern theory and applications of probability and stochastics. The style and coverage is geared towards the theory of stochastic processes, but with some attention to the applications. In many instances the gist of the problem is introduced in practical, everyday language and then is made precise in mathematical form. The first four chapters are on probability theory: measure and integration, probability spaces, conditional expectations, and the classical limit theorems. There follows chapters on martingales, Poisson random measures, Levy Processes, Brownian motion, and Markov Processes.Special attention is paid to Poisson random measures and their roles in regulating the excursions of Brownian motion and the jumps of Levy and Markov processes. Each chapter has a large number of varied examples and exercises. The book is based on the author’s lecture notes in courses offered over the years at Princeton University. These courses attracted graduate students from engineering, economics, physics, computer sciences, and mathematics. Erhan Çinlar has received many awards for excellence in teaching, including the President’s Award for Distinguished Teaching at Princeton University. His research interests include theories of Markov processes, point processes, stochastic calculus, and stochastic flows. The book is full of insights and observations that only a lifetime researcher in probability can have, all told in a lucid yet precise style.},
	language = {en},
	urldate = {2018-08-07},
	publisher = {Springer-Verlag},
	author = {Çınlar, Erhan},
	year = {2011},
	file = {Snapshot:C\:\\Users\\david\\Zotero\\storage\\I8JJCEJC\\9780387878584.html:text/html}
}


@article{fong_causal_2013,
	title = {Causal {Theories}: {A} {Categorical} {Perspective} on {Bayesian} {Networks}},
	shorttitle = {Causal {Theories}},
	url = {http://arxiv.org/abs/1301.6201},
	abstract = {In this dissertation we develop a new formal graphical framework for causal reasoning. Starting with a review of monoidal categories and their associated graphical languages, we then revisit probability theory from a categorical perspective and introduce Bayesian networks, an existing structure for describing causal relationships. Motivated by these, we propose a new algebraic structure, which we term a causal theory. These take the form of a symmetric monoidal category, with the objects representing variables and morphisms ways of deducing information about one variable from another. A major advantage of reasoning with these structures is that the resulting graphical representations of morphisms match well with intuitions for flows of information between these variables. These categories can then be modelled in other categories, providing concrete interpretations for the variables and morphisms. In particular, we shall see that models in the category of measurable spaces and stochastic maps provide a slight generalisation of Bayesian networks, and naturally form a category themselves. We conclude with a discussion of this category, classifying the morphisms and discussing some basic universal constructions. ERRATA: (i) Pages 41-42: Objects of a causal theory are words, not collections, in \$V\$, and we include swaps as generating morphisms, subject to the identities defining a symmetric monoidal category. (ii) Page 46: A causal model is a strong symmetric monoidal functor.},
	urldate = {2018-08-08},
	journal = {arXiv:1301.6201 [math]},
	author = {Fong, Brendan},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.6201},
	keywords = {Mathematics - Probability},
	annote = {Comment: 72 pages}
}


@article{ranganath_multiple_2018,
	title = {Multiple {Causal} {Inference} with {Latent} {Confounding}},
	url = {http://arxiv.org/abs/1805.08273},
	abstract = {Causal inference from observational data requires assumptions. These assumptions range from measuring confounders to identifying instruments. Traditionally, these assumptions have focused on estimation in a single causal problem. In this work, we develop techniques for causal estimation in causal problems with multiple treatments. We develop two assumptions based on shared confounding between treatments and independence of treatments given the confounder. Together these assumptions lead to a confounder estimator regularized by mutual information. For this estimator, we develop a tractable lower bound. To fit the outcome model, we use the residual information in the treatments given the confounder. We validate on simulations and an example from clinical medicine.},
	urldate = {2018-10-18},
	journal = {arXiv:1805.08273 [cs, stat]},
	author = {Ranganath, Rajesh and Perotte, Adler},
	month = may,
	year = {2018},
	note = {arXiv: 1805.08273},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv\:1805.08273 PDF:/home/users/u4533535/Zotero/storage/36YXHYZC/Ranganath and Perotte - 2018 - Multiple Causal Inference with Latent Confounding.pdf:application/pdf;arXiv.org Snapshot:/home/users/u4533535/Zotero/storage/ZF7FV6YS/1805.html:text/html}
}


@book{hernan_causal_2018,
	title = {Causal {Inference}},
	language = {en-us},
	publisher = {Chapman \& Hall/CRC},
	author = {Hernán, MA and Robins, JM},
	year = {2018},
	file = {Snapshot:/home/users/u4533535/Zotero/storage/3PD2MVZA/causal-inference-book.html:text/html}
}



@incollection{weirich_causal_2016,
	edition = {Winter 2016},
	title = {Causal {Decision} {Theory}},
	url = {https://plato.stanford.edu/archives/win2016/entries/decision-causal/},
	abstract = {Causal decision theory adopts principles of rational choice thatattend to an act’s consequences. It maintains that an account ofrational choice must use causality to identify the considerations thatmake a choice rational., Given a set of options constituting a decision problem, decisiontheory recommends an option that maximizes utility, that is, an optionwhose utility equals or exceeds the utility of every other option. Itevaluates an option’s utility by calculating the option’sexpected utility. It uses probabilities and utilities of anoption’s possible outcomes to define an option’s expectedutility. The probabilities depend on the option. Causal decisiontheory takes the dependence to be causal rather than merelyevidential., This essay explains causal decision theory, reviews its history,describes current research in causal decision theory, and surveys thetheory’s philosophical foundations. The literature on causaldecision theory is vast, and this essay covers only a portion ofit.},
	urldate = {2018-11-15},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Weirich, Paul},
	editor = {Zalta, Edward N.},
	year = {2016},
	file = {SEP - Snapshot:/home/users/u4533535/Zotero/storage/VFCIH54I/decision-causal.html:text/html}
}


@article{peters_structural_2013,
	title = {Structural {Intervention} {Distance} ({SID}) for {Evaluating} {Causal} {Graphs}},
	url = {http://arxiv.org/abs/1306.1043},
	abstract = {Causal inference relies on the structure of a graph, often a directed acyclic graph (DAG). Different graphs may result in different causal inference statements and different intervention distributions. To quantify such differences, we propose a (pre-) distance between DAGs, the structural intervention distance (SID). The SID is based on a graphical criterion only and quantifies the closeness between two DAGs in terms of their corresponding causal inference statements. It is therefore well-suited for evaluating graphs that are used for computing interventions. Instead of DAGs it is also possible to compare CPDAGs, completed partially directed acyclic graphs that represent Markov equivalence classes. Since it differs significantly from the popular Structural Hamming Distance (SHD), the SID constitutes a valuable additional measure. We discuss properties of this distance and provide an efficient implementation with software code available on the first author's homepage (an R package is under construction).},
	urldate = {2018-02-20},
	journal = {arXiv:1306.1043 [stat]},
	author = {Peters, Jonas and Bühlmann, Peter},
	month = jun,
	year = {2013},
	note = {arXiv: 1306.1043},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1306.1043 PDF:/home/users/u4533535/Zotero/storage/XKLD3IZ9/Peters and Bühlmann - 2013 - Structural Intervention Distance (SID) for Evaluat.pdf:application/pdf;arXiv.org Snapshot:/home/users/u4533535/Zotero/storage/BFJYMINB/1306.html:text/html}
}


@article{jeffrey_logic_1981,
	title = {The logic of decision defended},
	volume = {48},
	issn = {1573-0964},
	url = {https://doi.org/10.1007/BF01063989},
	doi = {10.1007/BF01063989},
	abstract = {The approach to decision theory floated in my 1965 book is reviewed (I), challenged in various related ways (II–V) and defended, firstad hoc (II–IV) and then by a general argument of Ellery Ells's (VI). Finally, causal decision theory (in a version sketched in VII) is exhibited as a special case of my 1965 theory, according to the Eellsian argument.},
	language = {en},
	number = {3},
	urldate = {2018-11-19},
	journal = {Synthese},
	author = {Jeffrey, Richard},
	month = sep,
	year = {1981},
	keywords = {Causal Decision, Causal Decision Theory, Decision Theory, General Argument},
	pages = {473--492},
	file = {Springer Full Text PDF:/home/users/u4533535/Zotero/storage/ZXMJH69T/Jeffrey - 1981 - The logic of decision defended.pdf:application/pdf}
}


@inproceedings{dawid_beware_2010,
	title = {Beware of the {DAG}!},
	url = {http://proceedings.mlr.press/v6/dawid10a.html},
	abstract = {Directed acyclic graph (DAG) models are popular tools for describing causal relationships and for guiding attempts to learn them from data.  They appear to supply a means of extracting causal concl...},
	language = {en},
	urldate = {2018-03-09},
	booktitle = {Causality: {Objectives} and {Assessment}},
	author = {Dawid, A. Philip},
	month = feb,
	year = {2010},
	pages = {59--86},
	file = {Full Text PDF:C\:\\Users\\david\\Zotero\\storage\\K2FLIM3E\\Dawid - 2010 - Beware of the DAG!.pdf:application/pdf;Snapshot:C\:\\Users\\david\\Zotero\\storage\\3BBKAGH9\\dawid10a.html:text/html}
}


@article{wu_toward_2018,
	title = {Toward an {AI} {Physicist} for {Unsupervised} {Learning}},
	url = {http://arxiv.org/abs/1810.10525},
	abstract = {We investigate opportunities and challenges for improving unsupervised machine learning using four common strategies with a long history in physics: divide-and-conquer, Occam's Razor, unification, and lifelong learning. Instead of using one model to learn everything, we propose a novel paradigm centered around the learning and manipulation of *theories*, which parsimoniously predict both aspects of the future (from past observations) and the domain in which these predictions are accurate. Specifically, we propose a novel generalized-mean-loss to encourage each theory to specialize in its comparatively advantageous domain, and a differentiable description length objective to downweight bad data and "snap" learned theories into simple symbolic formulas. Theories are stored in a "theory hub", which continuously unifies learned theories and can propose theories when encountering new environments. We test our implementation, the "AI Physicist" learning agent, on a suite of increasingly complex physics environments. From unsupervised observation of trajectories through worlds involving random combinations of gravity, electromagnetism, harmonic motion and elastic bounces, our agent typically learns faster and produces mean-squared prediction errors about a billion times smaller than a standard feedforward neural net of comparable complexity, typically recovering integer and rational theory parameters exactly. Our agent successfully identifies domains with different laws of motion also for a nonlinear chaotic double pendulum in a piecewise constant force field.},
	urldate = {2018-11-27},
	journal = {arXiv:1810.10525 [cond-mat, physics:physics]},
	author = {Wu, Tailin and Tegmark, Max},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.10525},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Physics - Computational Physics},
	annote = {Comment: Typos fixed, references added, discussion improved. 18 pages, 7 figs}
}


@article{gupta_intention--treat_2011,
	title = {Intention-to-treat concept: {A} review},
	volume = {2},
	issn = {2229-3485},
	shorttitle = {Intention-to-treat concept},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3159210/},
	doi = {10.4103/2229-3485.83221},
	abstract = {Randomized controlled trials often suffer from two major complications, i.e., noncompliance and missing outcomes. One potential solution to this problem is a statistical concept called intention-to-treat (ITT) analysis. ITT analysis includes every subject who is randomized according to randomized treatment assignment. It ignores noncompliance, protocol deviations, withdrawal, and anything that happens after randomization. ITT analysis maintains prognostic balance generated from the original random treatment allocation. In ITT analysis, estimate of treatment effect is generally conservative. A better application of the ITT approach is possible if complete outcome data are available for all randomized subjects. Per-protocol population is defined as a subset of the ITT population who completed the study without any major protocol violations.},
	number = {3},
	urldate = {2018-12-06},
	journal = {Perspectives in Clinical Research},
	author = {Gupta, Sandeep K.},
	year = {2011},
	pmid = {21897887},
	pmcid = {PMC3159210},
	pages = {109--112}
}