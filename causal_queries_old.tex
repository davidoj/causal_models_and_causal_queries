
% A proposed general framework for decision-theoretic causal queries:

% % \begin{definition}[Causal model query]\label{def:causal_query_1}
% % Given complete dataset $D$ over domain $V$ and a sample $S\subset D$, a class of causal models $\mathcal{M}$ and a $D$-dependent loss $\mathcal{L}_D:\mathcal{M}\to \mathbb{R}$, using the sample $S$ find a model $M\in \mathcal{M}$ minimising $\mathcal{L}_D$?
% % \end{definition}

% % Thinking aloud: given a distinction between intervention and other variables $V=X\times \mathcal{I}$, the causal model defined in \ref{def:causal_models} explicitly avoids requiring a marginal distribution $P(\mathcal{I})$, though the conditional $P(X|\mathcal{I}=I)=M(I)$ exists.

% % On the other hand, given a complete dataset $D$, $P(\mathcal{I})$ will exist and $\mathcal{L}_D$ will depend on it. It seems undesirable for a causal query to depend on the selection of intervention.

% % Try stepping up a level of generality:

% % Let $\mathcal{I}$ be an arbitrary set of interventions. If we imagine a robot interacting with the world, we can think of $\mathcal{I}$ as the set of all possible signals the robot could send to its actuators. We will later discuss when it makes sense to consider particular types of intervention such as $do$-type interventions.

% Define the collection of conditionals $P(X|\mathcal{I})=\{P(X|I)|I\in\mathcal{I}\}$.

% \begin{definition}[Causal policy]\label{def:causal_query_2}
% Given a set of conditionals $P(X|\mathcal{I})$ with domain $X$ and interventions $\mathcal{I}$ and a cost function $C:X\times\mathcal{I}\to\mathbb{R}$ find $I$ minimising $\mathbb{E}_{P(X|I)}[C(X,I)]$. The resulting $I$ is the \emph{causal policy}
% \end{definition}

% \subsection{Problems with causal policies}\label{ssec:prob_causal_policy}

% A causal policy (Def \ref{def:causal_query_2}) suggests the existence of an agent with an ability to conduct the interventions $\mathcal{I}$. However, definitions so far leave open the mechanism by which the agent can conduct such interventions. This is undesirable, as if the agent implements a policy using some mechanism $\mu_I$ to conduct intervention $I$, we might justifiably be concerned about whether $P(X|I,\mu_I)=P(X|I)$. This isn't just a theoretical concern - it is easy to come up with examples of situations where $P(X|I,do(I))\neq P(X|I,\neg do(I))$. If we add a node representing $\mu_I$ to the model, we might shift our concern to a potential metamechanism $\mu'_I$ and so forth. It might be intuitively obvious in many cases that we can stop when we reach a given level, but the lack of a formal account is unsatisfying.

% \begin{definition}[Causal optimization problem]\label{def:copt_prob}
% A causal optimization problem is a tuple $\langle \mathcal{I}, V, \mathcal{F}, m, C\rangle$. $V$ is a set of variables and $\mathcal{I}$ is a set of interventions and $V$ is a set of random variables. $\mathcal{F}$ is a set of functions $V\to \mathcal{I}$ and $m$ is a map $\mathcal{F}\to \mathcal{P}_{V\times \mathcal{I}}$, where $\mathcal{P}_{V\times \mathcal{I}}$ is a set of probability distributions on $V\times \mathcal{I}$. $C:V\times \mathcal{I} \to \mathbb{R}$ is a cost function.
% \end{definition}

% \subsection{Functional policy}

% My proposed solution is to incorporate a policy function into the world model. This demystifies what exactly "interventions" are - they are policy function outputs.

% \begin{definition}[Functional policy]\label{def:fp_cpq}
% Suppose we have a domain $X\times\mathcal{I}$, a set of candidate policies $\mathcal{F}:\{X\to \mathcal{I}\}$, a map $m:\mathcal{F}\to \mathcal{P}$ where $\mathcal{P}$ is the space of probability distributions over $X\times\mathcal{I}$ and a cost function $C:X\times\mathcal{I}\to \mathbb{R}$.

% The functional policy is
% \begin{align}
%     \Pi=\mathrm{argmin}_{f\in\mathcal{F}}\mathbb{E}_{m(f)}[C(x,I)]    \label{eq:func_cp}
% \end{align}
% \end{definition}

% As the codomain of the map $m$ is a space of probability distributions, I will use the notation
% \begin{align}
%     P_\Pi := m(\Pi)
% \end{align}

% Needs work: I think that the domain of functions in $\mathcal{F}$ should probably be restricted to things that happen before the function is computed. I'm reluctant to include time if it can be avoided, so leaving it as a note for now.

% Definition \ref{def:fp_cpq} is motivated by the following consideration: Suppose we have a true set of conditionals $P^*(X|\mathcal{I})=\{P^*(X|I):\forall I \in \mathcal{I}\}$. Deducing a policy from \ref{def:causal_query_2} using the model $P(X|\mathcal{I})=P^*(X|\mathcal{I})$ is not sufficient to ensure the chosen policy is actually optimal because we haven't specified a model of how the agent interacts with $\mathcal{I}$. Suppose, instead, we have a map $m^*:\mathcal{F}\to\mathcal{P}$ that is true for all $f\in\mathcal{F}$ and derive an optimal policy $\Pi\in\mathcal{F}$ following \ref{def:fp_cpq} with $m=m^*$. This is sufficient to ensure that $\Pi$ is superior to all other policies in $\mathcal{F}$.

% % \begin{remark}
% % If the set of distributions $\mathcal{P}$ are able to be represented by structural equation models, all variables are functions of other variables in the model. Intervention variables are then distinguished only by the freedom to choose which function computes them.
% % \end{remark}


% \subsubsection{Self-optimisation}\label{sssec:self_reflection}

% Finding an optimal policy via Definition \ref{def:fp_cpq} is itself the application of a map $Q:\{\mathcal{F}\}\times\{\mathcal{F}\to\mathcal{P}\}\times\{\mathcal{I}\to\mathbb{R}\}\to\{X\to\mathcal{I}\}$ such that $Q(\mathcal{F},m,C)=\Pi$ (with symbols defined as above). 

% Introducing a map $G\{X\to X\to \mathcal{I}\} \to \{X\to\mathcal{I}\}$, $G(Q)$ is itself a policy - in fact it is the policy actually implemented by the agent, while the idealised $\Pi:=Q(X)$ will, in general, forget some of the inputs of $G(Q)$. 

% It is interesting to ask whether it is possible to set the optimisation problem up so that $\Pi=G(Q)$, and what implications this might have. To do so, we must ensure that $G(Q)\in\mathcal{F}$, and so we must augment the domain $X$ with variables representing $\mathcal{F}$,$m$ and $C$.

% Needs work: the fact that the model $m$ must contain a representation of itself is suggestive of an incomputability result. Is there such a result here?

% Needs work: Generally, when is it possible to take a policy $\Pi_1:X\to\mathcal{I}$ and produce a simplified policy $\Pi_2:C\to\mathcal{I}$ where $C\subset X$ which achieves the same cost?


% \subsection{Functional policies in Bayesian Networks}

% The definitions above do not require the probability distributions $\mathcal{P}$ be Markovian. However, Markovian distributions permit easy representation of conditional independence with graphical models, so they make a good basis from which to generate examples.

% We will follow the custom of labeling nodes in a graphical model with the output of the function represented by the node, so the policy node will be labeled $I$.

% We will also neglect any subtleties raised by section \ref{sssec:self_reflection}.

% Needs work: the inclusion of a policy function suggests that $I$ may be locally Markovian with respect to the inputs to the policy function. Need to check the exact characteristics WRT Markovianity.

% \subsubsection{Simple case}\label{sssec:simple_case}

% Let $\mathcal{I},X=\{0,1\}$ and $C(x,I)=x$.

% \begin{center}
% \begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
% semithick ,
% state/.style ={ circle ,top color =white ,
% draw , text=blue , minimum width =1 cm}]
% \node[state] (A) [left] {$I$};
% \node[state] (B) [right = of A] {$X$};
% \path (A) edge [] node[above] {} (B);
% \end{tikzpicture}
% \end{center}

% We will partially define the probability distribution with the following structural equation:
% \begin{align*}
%     X:=I
% \end{align*}
% Choosing a function that computes $I$ will complete the definition.

% Given the dependences indicated, we have as $\mathcal{F}$ the set of constant functions on $\mathcal{I}$. Of these, $I\mapsto0$ minimises $C$.

% In this simple case, finding the functional policy reduces to finding the best intervention. 

% \subsubsection{Additional dependence}

% Let $\mathcal{I},X_1,X_2=\{0,1\}$ and $C(x_1,x_2,I)=x_1$. Suppose $X_2$ is observed before $I$ is computed, so it is a potential input of the policy.

% \begin{center}
% \begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
% semithick ,
% state/.style ={ circle ,top color =white ,
% draw , text=blue , minimum width =1 cm}]
% \node[state] (B) [right] {$X_1$};
% \node[state] (A) [above left = 1cm and 3cm of B] {$X_2$};
% \node[state] (C) [below left = 1cm and 3cm of B] {$I$};
% \path (A) edge [] node[above] {} (B);
% \path (C) edge [] node[above] {} (B);
% \draw[dashed] (A) -- (C);
% \end{tikzpicture}
% \end{center}

% We will partially define the distribution with:
% \begin{align*}
%     X_1 := \mathrm{XOR}(I,X_2)
% \end{align*}

% We could consider policies that are constant on $\{0,1\}$, or maps $\{0,1\}\to\{0,1\}$. Clearly in this case we want $\Pi:I\mapsto X_2$. If we further knew that $P(X_2=0)=1$, then the constant policy $f:I\mapsto 0$ would perform equally well.

% \begin{remark}
% The optimal policy isn't straightforward to derive using Causal Bayesian Networks. If we consider the setup above where we are permitted to conduct interventions on the node labeled by $I$, under the standard definition of intervention we would assume that it cuts off any dependence of $I$ on $X_2$. This would limit us to what are here described as constant policies, which are not in general optimal. 

% We could address this by proposing that $do()$-interventions could depend on the value of $X_2$ even if $I$ does not depend on it directly. This would invalidate some properties of Causal Bayesian Networks.
% \end{remark}

% \begin{remark}
% If the policy depends on $X_2$, we never need to know the full probability distribution to find the optimal policy.
% \end{remark}

% \subsubsection{Hidden dependence}
% Let $\mathcal{I},X_1,X_2=\{0,1\}$ and $C(x_1,x_2,I)=x_1$. Suppose $X_2$ is not observed.

% \begin{center}
% \begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
% semithick ,
% state/.style ={ circle ,top color =white ,
% draw , text=blue , minimum width =1 cm}]
% \node[state] (B) [right] {$X_1$};
% \node[state,dashed] (A) [above left = 1cm and 3cm of B] {$X_2$};
% \node[state] (C) [below left = 1cm and 3cm of B] {$I$};
% \path (C) edge [] node[above] {} (B);
% \draw[dashed] (A) -- (B);
% \end{tikzpicture}
% \end{center}

% We will assume the following structural equations
% \begin{align*}
%     X_2 &:= \mu_{X_2}\\
%     X_1 &:= \mathrm{XOR}(I,X_2)
% \end{align*}
% Where $\mu_{X_2}\sim \mathrm{Bernoulli}(0.2)$.

% In this case, the problem appears to be identical to a problem with the dependence structure of \ref{sssec:simple_case}, and the optimal policy is constant $\Pi:I\mapsto 1$.

% \subsubsection{Confounded observations}
% Let $\mathcal{I},X_2=\{0,1\}$, $X_1=\{0,1,2\}$ and $C(x_1,x_2,I)=x_1$. Suppose $X_2$ is not observed.

% Suppose also that we make infinite observations under some fixed policy $f$, and we are then free to impose our preferred policy $\Pi$.

% \begin{center}
% \begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
% semithick ,
% state/.style ={ circle ,top color =white ,
% draw , text=blue , minimum width =1 cm}]
% \node[state] (B) [right] {$X_1$};
% \node[state,dashed] (A) [above left = 1cm and 3cm of B] {$X_2$};
% \node[state] (C) [below left = 1cm and 3cm of B] {$I$};
% \path (C) edge [] node[above] {} (B);
% \draw[dashed] (A) -- (B);
% \draw (A) -- (C);
% \end{tikzpicture}
% \end{center}

% We will assume the following structural equations
% \begin{align*}
%     X_2 &:= \mu_{X_2}\\
%     X_1 &:= \neg I + X_2 \\
%     f   &:= X_2
% \end{align*}
% Where $\mu_{X_2}\sim \mathrm{Bernoulli}(0.2)$.

% The optimal policy will be $\Pi:I\mapsto 0$, but from our observations it will appear that $X_1 \CI I$ which would suggest that $\Pi$ is no better than the alternative.

% \subsubsection{Randomisation}
% Let $\mathcal{I}\{0,1\}$, $X_1=\{0,1,2\}$, $N=\{0,1,..,i,..,n\}$ and $C(x_1,i,I)=x_1$. Suppose $X_2$ is not observed.

% We wish to estimate the distribution of $X_1$ under fixed policies $f_0:I\mapsto 0$ and $f_1:I\mapsto 1$, and we can sample the system $n$ times. In order to do this, we will in fact be sampling under the policy $r:N\mapsto \{0,1\}$.

% \begin{center}
% \begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
% semithick ,
% state/.style ={ circle ,top color =white ,
% draw , text=blue , minimum width =1 cm}]
% \node[state] (B) [right] {$X_1$};
% \node[state,dashed] (A) [above left = 1cm and 3cm of B] {$N$};
% \node[state] (C) [below left = 1cm and 3cm of B] {$I$};
% \path (C) edge [] node[above] {} (B);
% \draw[dashed] (A) -- (B);
% \draw (A) -- (C);
% \end{tikzpicture}
% \end{center}

% The dependence structure is identical to the previous section, which showed that in general it isn't possible to deduce the optimal policy from these observations. The difference is that we are free to choose $r$.

% Suppose there is some set of ``plausible'' functions $\mathcal{H}=\{N\times I\to X_1\}$. If we choose $f'$ such that 
% \begin{align}
%     \max_{h\in\mathcal{H}} \left| \langle \{h(i,0):r(i)=0\}\rangle - \langle \{h(i,0):i\in\{n\}\} \rangle \right| < \epsilon \label{eq:bin_random}
% \end{align}
% Where $\langle\cdot\rangle$ denotes an average. If we suppose this also holds for $h(\cdot,1)$, then we can be confident that the subset of samples for which $r(i)=0$ will provide a good estimate for the distribution of $X_1$ under $f_0$, and the subset for which $r(i)=1$ will provide a good estimate for the distribution under $f_1$.

\subsection{Identifiability}

\subsubsection{Causal Bayesian Networks}

In the Pearlean framework, the causal effect of $X$ on $Y$ is identifiable if $P(y|do(X=i))$ is able to be consistently estimated from infinite observational data for all $i\in \mathrm{Range}(X)$. Given a graph $G$ over vertices $V$ representing a CBN (def \ref{def:CBN}) and a subset $X\subset V$, denote the graph deleting all outgoing edges of $X$ by $G_{\underline{X}}$ and the graph obtained by deleting all incoming edges to $X$ as $G_{\overline{X}}$.

Under these assumptions, four conditions for the identifiability of $X$ on $Y$ are given that are claimed to be both necessary and sufficient. The faithfulness assumption is not mentioned in the reference, but I can provide an example of an identifiable effect where $P$ is minimial but not faithful to $G$ and the following conditions do not hold \cite{galles_testing_2013}. Recall d-separation (definition \ref{def:dsep}) for the points below:

\begin{enumerate}
    \item There is no path from $X$ to $Y$ in $G$
    \item (No back-door path) $X\perp_{G_{\underline{X}}} Y$
    \item (All back-door paths blocked) If $Z\subset V$ are observed, then $X\perp_{G_{\underline{X}}} Y|Z$
    \item (``Front-door'' criterion) There exists $Z_1, Z_2 \subset V$ such that
    \begin{itemize}
        \item $Z_2\cap X =\varnothing$
        \item $Z_1$ blocks all directed paths from $X$ to $Y$: $X \perp_{G_{\overline{X}}} Y | Z_1$
        \item $Z_2$ blocks all back-door paths between $Z_1$ and $Y$ in $G_{\overline{X}}$: $Z_1 \perp_{G_{{\overline{X}}{\underline{Z_1}}}} Y | Z_2$
        \item $Z_2$ blocks all back-door paths between $X$ and $Z_1$: $Z_1 \perp_{G_{{\underline{X}}}} X | Z_2$
    \end{itemize}
\end{enumerate}

Recall that $d$-separation statements imply conditional independences in the associated probability distribution.

\subsubsection{Challenges for the CBN account of identifiability}

\textbf{No assumption of common support} 

It's easy to verify that the probability distribution generated by $P(I=i)=\delta(i)$, $V=I$ is a CBN compatible with the following graph that satisfies condition 2:

\begin{center}
\begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
semithick ,
state/.style ={ circle ,top color =white ,
draw , text=blue , minimum width =1 cm}]
\node[state] (B) [right] {$V$};
\node[state] (A) [left = 3cm of B] {$I$};
\path (A) edge [] node[above] {} (B);
\end{tikzpicture}
\end{center}

However, this doesn't allow for $P(V|do(I=1))$ to be estimated, as $I=1$ is inconsistent with the assumed distribution of $I$.

\textbf{Difficulties modelling interactions}

Suppose a vaccine for a contagious disease is being tested on two individuals who live in the same home. The probability of infection $D$ depends on both the vaccination decision $V$ and the presence of another infected person $P$:

\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c}
        & $P=0$ & $P=1$ \\
         \hline
        $V=0$ & 0.5 & 1 \\
        $V=1$ & 0.2 & 0.4
    \end{tabular}
    \caption{Probability of disease $D$ by vaccination and presence of infection}
    \label{tab:vaccination_interaction}
\end{table}

For simplicity's sake, we will posit there are two opportunities for infection, both governed by the probabilities in the above table. At the first infection opportunity, neither housemate is infected. However, the presence of infection is only measured once, at the end of the second period, and individuals don't recover once infected.

If the distribution of $V$ is either $P(V=1)=1$ or $P(V=1)=0$, the following graph could be argued to be a valid CBN for the distribution defined above:

\begin{center}
\begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
semithick ,
state/.style ={ circle ,top color =white ,
draw , text=blue , minimum width =1 cm}]
\node[state] (A) [left] {$V$};
\node[state] (B) [right = 3cm of A] {$D$};
\path (A) edge [] node[above] {} (B);
\end{tikzpicture}
\end{center}

It is frequently mentioned that the $P(Y|do(X=x))$ should be able to be thought about as the distribution of $Y$ given that $X$ was set to $x$ \emph{by a randomised experiment}, which is not true in this case.

The situation can be correctly modeled with a CBN which captures the dynamics. Modelling the dynamics is necessary to avoid cycles in the causal graph. Here $D^i_j$ is the infection state of housemate $i$ at time $j$:

\begin{center}
\begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
semithick ,
state/.style ={ circle ,top color =white ,
draw , text=blue , minimum width =1 cm}]
\node[state] (A) [left] {$V^1$};
\node[state] (F) [below = 2cm of A] {$V^2$};
\node[state] (B) [right = 3cm of A] {$D^1_1$};
\node[state] (C) [right = 3cm of F] {$D^2_1$};
\node[state] (D) [right = 3cm of B] {$D^1_2$};
\node[state] (E) [right = 3cm of C] {$D^2_2$};
\draw (A) -- (B);
\draw (F) -- (C);
\draw (B) -- (E);
\draw (C) -- (D);
\draw (B) -- (D);
\draw (C) -- (E);
\end{tikzpicture}
\end{center}

However, it seems legitimate to be interested $P(D^1,D^2|V^1,V^2)$, and it is possible to find this without needing to posit a model of the dynamics.

\subsubsection{Potential Outcomes}

The potential outcomes approach employs substantially different terminology to the CBN view of causality. It is specified through a mixture of formal definition and example, and I'm going to reproduce that here because I want to avoid imposing my own ideas about formal definitions on it.

Suppose a randomised trial with two treatment options $T\in\{0,1\}$ is administered to an infinite number of participants. For each participant $i\in\mathbb{N}$ there is a random variable $Y_i$ indicating the outcome of that participant. There are random variables $Y_i(0)$ and $Y_i(1)$ indicating the outcome participant $i$ would have experienced had they not received or received treatment respectively. For each participant there is an extra variable $X_i$ consisting of extra features related to the administration (or not) of the treatment to participant $i$. Finally there is a set of variables $W_i$ indicating the treatment assignment of participant $i$.

Given this setup, the potential outcomes approach often aims to estimate $\langle Y(1) \rangle - \langle Y(0) \rangle$ where $\langle \cdot \rangle$ indicates an average over every participant. 

There are a few assumptions given as necessary to be able to estimate this quantity:

\textbf{Stable unit treatment value assumption}: for any $i$, the values of $Y_i(0)$ and $Y_i(1)$ don't depend on any function of the set of treatment assignments $\{W_i:i\in\mathbb{N}\}$. I'm editorializing here, Rubin puts it this way: "First, it assumes that there is no interference between units (Cox 1958); that is, neither Y i (1) nor Y i (0) is affected by what action any other unit received. Second, it assumes that there are no hidden versions of treatments; no matter how unit i received treatment 1, the outcome that would be observed would be Y i (1) and similarly for treatment 0." 

\textbf{Ignorability}: Define $Y_{obs}(1)=W_i Y_i(1)$ and $Y_{obs}(0)= (1-W_i)Y_i(0)$. Ignorability holds that \begin{align}
    P(Y_i(1)|W_i=1,X_i=x) &= P(Y_i(1)|X_i=x) \\
    P(Y_i(0)|W_i=0,X_i=x) &= P(Y_i(0)|X_i=x)
\end{align}

\textbf{Common support}: Common support is the assumption that for all $w\in \mathrm{Range}(W_i)$, $x\in \mathrm{Range}(X_i)$
\begin{align}
    P(W_i=w|X_i=x) > 0
\end{align}

\subsubsection{Challenges for the Potential Outcomes approach to identifiability}

\textbf{Inaccessibility of the ignorability assumption}

The ignorability assumption can't be tested, as $Y_i(0)$ is not known in every case. This is problematic in particular because there are few cases besides randomised control trials in which a strong argument can be made that it holds.

A typical approach to solving the problem outside of RCTs is to add a large vector of covariates $X$ and hope that this causes ignorability to hold. It's not clear when this might or might not be a sound approach.

\textbf{Ambiguity over how to adjust for $X$}

Consider the following two cases, and assume extended ignorability:

\begin{center}
\begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
semithick ,
state/.style ={ circle ,top color =white ,
draw , text=blue , minimum width =1 cm}]
\node[state] (B) [right] {$V$};
\node[state] (A) [left = 3cm of B] {$I$};
\node[state] (C) [above right = 1cm and 1.5cm of A] {$X$};
\path (A) edge [] node[above] {} (B);
\draw (C) -- (A);
\draw (C) -- (B);
\node[state] (D) [right = 4cm of B] {$I$};
\node[state] (E) [right = 1.5cm of D] {$X$};
\node[state] (F) [right = 1.5cm of E] {$V$};
\path (A) edge [] node[above] {} (B);
\draw (C) -- (A);
\draw (C) -- (B);
\draw (D) -- (E);
\draw (E) -- (F);
\end{tikzpicture}
\end{center}


In the first case,
\begin{align}
    P(V|\phi)&=\sum_{I,X} P(X) P(I|X,\phi) P(V|I,X,\phi)\\
             &=\sum_{I,X} \phi(X)(I) P(X)  P(V|I,X,\phi_{obs}) \label{eq:hc_xcov}
\end{align}
while in the second case
\begin{align}
    P(V|\phi)&=\sum_I P(I|\phi) P(V|I,\phi) \\
             &=\sum_I \phi(\varnothing)(I) P(V|I,\phi_{obs})\\
             &=\sum_{I,X} \phi(\varnothing)(I) P(X|I,\phi_{obs}) P(V|I,X,\phi_{obs}) \label{eq:int_xcov}
\end{align}
Equations \ref{eq:hc_xcov} and \ref{eq:int_xcov} are not in general equal. Rubin recommends not adjusting for effects of the intervention, but this is somewhat ad-hoc as the potential outcomes approach doesn't actually define what an "effect of an intervention" is \cite{rubin_causal_2005}.

\subsubsection{Policy based identifiability}

Here is working definitions of causal models and causal identification problems:

\begin{definition}[Causal Model]
A causal model is a tuple $\mathcal{M} = \langle V, \Phi, P_\Phi \rangle$. $V$ is a set of variables with subsets $A, I\subset V$, and $\Phi$ is a set of Markov kernels $\phi:A\to \Delta I$. $P_\phi : \Phi\to \Delta V$ is a map from $\Phi$ to distributions on $V$. Even though $\Phi$ is not a random variable, we will write $P(V|\phi) := P_\phi(V)$.
\end{definition}

\begin{definition}[Causal identification problem]\label{def:causal_ident_prob}
A causal identification problem is a problem of  the form: Given a set of variables $V$, policies $\Phi$ and  whose joint dynamics are governed by an unknown causal model $\mathcal{M}$, find $P(V|\phi)$ for each $\phi\in \Phi$.
\end{definition}

The policy $\Phi$ can be understood to be a generalisation of $do(X=x)$ to a ``conditional do''. Under $do(X=x)$ a CBN assigns probability 1 to $X=x$. A Markov kernel $\phi$ can instead assign a range of distributions to $X$ which may depend on other variables.

A Markov kernel $\phi$ with the unit number $i$ in its domain can also be identified with the assignment variable $W_i$ in the potential outcomes approach. 

Under these conditions, it is possible to identify a causal model if three assumptions hold:

\textbf{Policy exchangeabilty}: $P(V\setminus\{A,I\}|A,I,\phi)=P(V\setminus\{A,I\}|A,I,\phi')$ for all $\phi,\phi' \in \Phi$

\textbf{Common support}: There exists a policy $\phi_{obs}\in\Phi$ for which $\phi_{obs}(a,i)>0$ for all $a\in A$, $i\in I$.

\textbf{Domain independence}: For all $\phi\in \Phi$, $P(A|\phi)=P(A|\phi')\overset{\mathrm{def}}{=}P(A)$

Issue: is domain independence necessary?

\begin{proof}
For any $\phi\in\Phi$,
\begin{align}
    P(V|\phi)&=\sum_{a}\sum_{i} P(V|a,i,\phi)\phi(a,i)P(a|\phi) \\
            &= \sum_{a}\sum_{i} P(V|a,i,\phi_{obs})\phi(a,i)P(a)
\end{align}

$P(V|a,i,\phi_{obs})$ and $P(a)$ can be estimated from i.i.d. observations of $V$ under $\phi_{obs}$ and $\phi(a,i)$ is known.

\end{proof}

The difference between this approach and the potential outcomes approach is that $\Phi$ is a set of Markov kernels with known domain $A$. This deals with the adjustment ambiguity. 

It's not quite clear how it compares to a CBN. It avoids assuming all variables obey a causal Markov condition, and so it has no problem estimating $P(D^1,D^2|V^1,V^2)$ without adding any extra dynamics in the vaccination case above. I think there might be something like a local Markov condition for the variable $I$ with parent set $A$, but I need to look into this further.

\subsection{Appendix: definitions}


\begin{definition}[Head-to-head node]
A head to head or collider node $V_{h}$ in a path $p$ is a node where the two adjacent edges $E_i$ and $E_{i+1}$ in $p$ are oriented as $V_{h-1}\rightarrow V_h$ and $V_h\leftarrow V_{h+1}$ respectively.
\end{definition}

\begin{definition}[Blocked path]
A path $p$ is blocked by a set $S\subset V$ iff $p$ contains a node that is not a head-to-head node that is also in $S$, or if $p$ contains a head-to-head node which is itself in $S$ or has a descendent in $S$.
\end{definition}

\begin{definition}[d-separation]\label{def:dsep}
Two sets of nodes $A,B\subset V$ are d-separated in $G$ by $S\subset V$ iff every path starting with a node $A_i\in A$ and ending with a node $B_i\in B$ is blocked by $S$. We write this relation $A \perp_G B | S$.
\end{definition}


\begin{definition}[Compatibility]\label{def:compatibility}
A probability distribution $P$ is compatible with a DAG $G$ iff $A\perp_G B |S \Rightarrow A\CI_P B | S$ for all $A, B, S \subset V$.
\end{definition}


\begin{definition}[Causal Bayesian Network]\label{def:CBN}
Consider the set of interventions $\mathcal{I}$ and a causal model $M:\mathcal{I}\to\mathcal{P}$ over random variables $V$, and write $P_A=M(I_A)$. $M$ is a Causal Bayesian Network if
\begin{enumerate}
    \item There is some cause-effect graph $G=(V,E)$ such that $M$ is Markovian relative to $G$ and
    \item For intervention $I_U$ and any $V_i \in \{V_k|k\in U\}^C$, $P_U(\mathrm{dc}_i) > 0 \implies P_U(V_i|\mathrm{dc}_i) = P_0(V_i|\mathrm{dc}_i)$ where $\mathrm{dc}_i$ is defined relative to $G$
    \item $I_U = \cup_{i\in U}[i,v_i] \implies P_U(\{V_i|i\in U\}) = \prod_{i\in U} \delta_{V_iv_i}$
\end{enumerate}
Here $\delta_{ij}$ is the Kronecker delta function, defined such that $\delta_ij=0$ for $i\neq j$ and $\delta_{ii} = 1$.

Call any graph $G$ for which properties 1 \& 2 hold the \emph{associated graph}.
\end{definition}

% The first three are sufficient for estimating $P_{\Pi_i}(y)$ from $P(y|I,z)$, while the fourth has a more complicated estimation procedure.

% In the Rubin Causal Model, the set of interventions considered is two-valued: $\mathcal{I}=\{0,1\}$. Rubin uses notation that considers finite sample effects and adds assumptions to ensure that we can derive IID samples from $P_{\Pi_0}$ and $P_{\Pi_1}$, which we will neglect. Rubin shows that the population average treatment effect $\mathbb{E}_{P_{\Pi_1}} [Y] - \mathbb{E}_{P_{\Pi_0}} [Y]$ can be calculated from samples from $P_{\Pi_o}(Y|I=0)$ and $P_{\Pi_o}(Y|I=1)$ provided \emph{ignorability} holds \cite{rubin_causal_2005}.

% To define ignorability properly, I'd need to define counterfactual quantities. For the moment I will use the following definition that I believe captures the same idea
% \begin{align}
%     P_{\Pi_o}(Y|Z,I=0) &= P_{\Pi_0}(Y|Z,I=0) \\
%     P_{\Pi_o}(Y|Z,I=1) &= P_{\Pi_1}(Y|Z,I=1)
% \end{align}

% Needs work: the difficulty is that counterfactuals posit a "true" value of $Y(1)$ - the outcome given the treatment. It seems \emph{reasonable} to interpret this as the value of $Y$ under a constant policy of $I\mapsto 1$, but it's not obvious that this is the only possible interpretation. The definition here does at least match Pearl's definition of counterfactuals.

% Pearl's identifiability condition is stronger than Rubin's. If we know $P_{\Pi_i}(y)$ for all $i\in \mathcal{I}$, then provided $P_{\Pi_i}$ has an expected value, we can trivially find $\mathbb{E}_{P_{\Pi_i}} [Y] - \mathbb{E}_{P_{\Pi_j}} [Y]$ for any $i,j\in\mathbb{R}$.

% Neither set of assumptions is strictly stronger than the other. (1)-(3) of the do-calculus identifiability imply ignorability, but (4) may hold when ignorability does not. On the other hand, given a graph where an unobserved variable $U$ is a parent of both $X$ and $Y$ (and hence violating the back-door criterion), we can propose a compatible probability distribution $P$ where ignorability holds due to intransitivity unfaithfulness at $U$ \ref{def:intrans_unfaith}.

% \textbf{Conjecture:} Rewriting (1)-(4) of the do-calculus conditions in terms of conditional independences rather than d-separation would render them necessary and sufficient to identify a causal effect.

% \subsubsection{Relationship between causal identification and cost minimisation}

% Both criteria for causal identification above consider only constant policies. Some questions arise:
% \begin{itemize}
%     \item Is causal identifiability along with the assumption that there is an optimal constant policy enough to enable the optimal policy to be found?
%     \item Is causal identifiability (or a modification of it) enough to enable an optimal non-constant policy to be found?
% \end{itemize}

% \subsubsection{Identifiability and constant policy cost minimisation}

% Suppose we have a causal optimization problem $\langle \mathcal{I}, V, \mathcal{F}, P_f, C\rangle$ where $\mathcal{F}$ is the set of constant functions $\Pi_i$ on $\mathcal{I}$. Let $Y\subset V\times\mathcal{I}$ be the set of variables on which $C$ is not constant. Then it is sufficient for the effect of $\mathcal{I}$ on $Y$ to be identifiable in the Pearlean sense for the optimal policy to be found (if it exists). Also sufficient is if the effect of $\mathcal{I}$ on $C(Y)$ to be identifiable.

% In both cases it is possible to evaluate the function $h(i):= E_{\Pi_i}[C(Y)]$ at every $i\in\mathcal{I}$, provided this expectation exists. If it is possible to minimize $h$ on $\mathcal{I}$, then an optimal policy can be found.

% Using Rubin's weaker definition of identifiability, it is necessary that the effect of $\mathcal{I}$ on $C(Y)$ be identifiable.

% \subsubsection{Identifiability and nonconstant policy cost minimisation}

% For reasons analogous to those above, if we admit non-constant policies $f:A\to I$ into consideration, it is sufficient to know $P_{f}(y|A=a)$ for all $f\in\mathcal{F}$.

% Suppose that the intervention $I$ has no causal effect on $A$. That is, $P_f (A) = P_{f'}(A)$ for any $f,f'\in\mathcal{F}$. This avoids issues of circular dependence, but it is an additional assumption, not a matter of necessity.

% If the above assumptions holds, $m$ is a CBN and the effect of $I$ on $Y$ is identifiable, then the conditional effect of $I$ on $Y$ given $A$ is identifiable.

% \textbf{Proof sketch:} First, redo Definition \ref{def:CBN} with conditional interventions.

% Next, by parental adjustment, $P_*(y|\mathrm{pa}_Y)$ is constant for all policies $*$ provided $\mathrm{pa}_Y$ has nonzero probability under $\mathrm{pa}_Y$.  

% Define $\mathrm{ndp}^Y_I = \mathrm{pa}_Y\cap \mathrm{nd}_I$ and $\mathrm{dep}^Y_I=\mathrm{pa}_Y\cap \mathrm{de}_I$

% \begin{align}
%     P_*(y|a) &= \sum_{\mathrm{pa}_Y\setminus \{A\}} P_*(y|\mathrm{pa}_Y) P_*(\mathrm{pa}_Y\setminus\{A\}|a) \\
%     P_*(y|a) &= \sum_{\mathrm{pa}_Y\setminus \{A\}} P_*(y|\mathrm{pa}_Y) P_*(\mathrm{ndp}^Y_I|a) P_*(\mathrm{dep}^Y_I|\mathrm{ndp}^Y_I,a)
% \end{align}

% We can split $\mathrm{pa}_Y$ into $\mathrm{pa}_Y\cap \mathrm{de}_I$ and $\mathrm{pa}_Y\cap \mathrm{nd}_I$. $P_*(\mathrm{ndp}_I)$ can only depend on the policy via $a$, and so $P_*(\mathrm{ndp}_I)$ is fixed for all policies. By the assumption that $A\in\mathrm{nd}_I$,  $P_*(\mathrm{dep}^Y_I|\mathrm{ndp}^Y_I,a)$ depends only on the policy via $I$.

% Therefore $P_f(y|a)=P_{f'}(y|a)$ for all $f,f'$ such that $f(a)=f'(a)$.

% If the effect of $I$ on $Y$ is identifiable, then we know $P_{\Pi_i}(y|a)$ for all constant policies $\Pi_i$. We can therefore compute $P_f(y|a)$ for non-constant $f$ by evaluating the constant policy $P_{\Pi_{f(a)}}(y|a)$ at every value of $a$.

% The weaker assumption of ignorability needs to be modified to
% \begin{align}
%     P_{\Pi_o}(Y|Z,I,A) &= P_{f}(Y|Z,I,A) \\
% \end{align}

% This isn't a straightforward change - if it were possible to declare probability distributions equal under arbitrary policies, causal modelling would be pointless.
